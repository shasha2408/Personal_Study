{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50dda428",
   "metadata": {},
   "source": [
    "# What is Feature Engineering \n",
    "\n",
    "**1. Learning What? :** \n",
    "\n",
    "- determine which features are the most important with mutual information.\n",
    "- invent new features in several real-world problem domains.\n",
    "- encode high-cardinalir categoriew with a target encoding. \n",
    "- create segmentation features with k-means clustering\n",
    "- decompose a dataset's variation into feature with principal component analysis.\n",
    "\n",
    "**2. The Goal of Feature Engineering :** \n",
    "\n",
    "The goal of feature engineering is simply to make your data better suited to the problem at hand.  \n",
    "\n",
    "Consider \"apparent temperature\" measures like the heat index and the wind chill. These quantities attempt to measure the perceived temperature to humans based on air temperature, humidty, and wind speed, things which we can measure directly. You could think of an apparent temperature as the result of a kind of feature engieerning, an attemp to make the observed data more relevant to what we actually care about. \n",
    "\n",
    "- improve a model's predictive performance\n",
    "- reduce computational or data needs\n",
    "- improve interpretability of the results \n",
    "\n",
    "**3. A Guiding Priciple of Feature Engineering :**\n",
    "\n",
    "For a feature to be useful, it must have a relationship to the target that your model is able to learn. Linear model, for instance, are only able to learn leiner relationship. So, when using a linear model, your goal is to transform the features to make their relationship to the target linear.  \n",
    "\n",
    "The key idea here is that a transformation you apply to a features becomes in essence a part of the model itself. Say you were trying to predict the Price of square plots of land from the Length of one side. Fitting a linear model directly ot Length gives poor results.   \n",
    "\n",
    "If we square the Length features to get \"Area\", however, we create a linear relationship. Adding Area to the feature set means this linear model can now fit a prarbola. Squaring a feature, in other words, gave the linear model the ability to fit squared features.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e3120",
   "metadata": {},
   "source": [
    "**4. Example - Concrete Formulations**\n",
    "\n",
    "To illustrate these ideas we'll see how adding a few synthetic feqtures to a dataset can improve the predictive performance of a randomforest model.  \n",
    "\n",
    "The Concrete datasets contains a variety of concrete formulations and resulting products's compressive strength, which is a measure of how much load that kind of concrete cen bear. The task for dataset is to predict a concrete's compressive strength given its formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da179a4a",
   "metadata": {},
   "source": [
    "# Mutual Infromation\n",
    "\n",
    "First encountering a new datasets can sometimes feel overwhelming. ou might be preseneted with hundreds or thousands of features without even a description to go by. Where do you even begin?  \n",
    "\n",
    "A great first step is to construct a ranking with a feature utility metric, a function measuring associations between a feature and the target. Then you can choose a smaller set of the most useful feture and the target. Then you cna choose a smaller set of the most useful features to develop initally and have more confidence that your time will be well spent.  \n",
    "\n",
    "The metric we'll use is called \"mutual information\". Mutual information is a lot like correlation in that it mesures a relationship between two wuantities. The advantage of mututal information is that it can detect any kind of relationship, while correlation only detects linear relationships.  \n",
    "\n",
    "Mutual information is a great general-purpose metric and especially useful at the start of feature development when you might not know what model you'd like to use yet. It is : \n",
    "\n",
    "- easy to use and interpret,\n",
    "- computationally efficient,\n",
    "- theoretically well-founded,\n",
    "- resistant to overfitting, and, \n",
    "- able to detect any kind of relationship\n",
    "\n",
    "**1. Mutual information and what it measures**\n",
    "\n",
    "Mutual information descirbes relationships in terms of uncertainty. The mutual information between two quantities is a meausre of the extent to which knowledge of one qulitiy reduces uncertainty aobut the other. If you knew the value of a feature, how much more confident would you be about the target?\n",
    "\n",
    "\n",
    "**2. Interpreting Mutual information Scores**\n",
    "\n",
    "The least possible mutual information between quantities is 0.0. When MI is zero, the quantities are independent : neither can tell you anything about the other. Coversly, in theroy there's no upper bound to what MI can be. In practice though values above 2.0 or so are uncommon. \n",
    "\n",
    "Here are some things to remember when applying mutual information : \n",
    "- MI can help you to understand the relative potential of a features as a predictor of the target, considered by itself. \n",
    "- If's possible for a feature to be very informative when interacting with other features, but not so informative all alone. MI can't detect interactiosn between features. It is a univariate metric.\n",
    "- The actural suefulness of a feature depends on the model you use it with. A feature is only useful to the extent that its relationship with the target is one your model can learn. Jsut becuase a feature has high MI score doesn't mean you model will be able to do anything with that information. you may need to transform the feature first toe expose the association\n",
    "\n",
    "**3. What need to focus**\n",
    "\n",
    "1. The scikit-learn algorithm for MI treats discrete features differently from continuous features. Consequently, you need to tell it which are which. As a rule of thumb, anything that must ahve a flaot dtypes is not discrete. Categorical can be treated by giving them a label encoding(or ordinaly encoding). \n",
    "\n",
    "2. Scikit-learn has two mutual information metrics in its feature_selection module: one for real-valued targets(mutaul_info_regresion) and one for categorical targets(mutual_info_classif)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f46bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6cb54",
   "metadata": {},
   "source": [
    "Data visulaization is a great addition to your feature-engineering toolbox. Along with utility metrics like mutual information, visualization like these can help you discover important relationships in your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cd0154",
   "metadata": {},
   "source": [
    "# Creating Features \n",
    "\n",
    "Once you've identified a set of features with some potential, it's time to start developing them. In this lesson, you'll learn a number of common transformation you can do entirely in Pandas.\n",
    "\n",
    "**1. Tips on Discovering New Features**\n",
    "\n",
    "- Understand the features. Refer to your dataset's data documentation, if available. \n",
    "- Reserach the problem domain to acquire domain knowledge. If your problem is predicting house prices, do some research on real-estate for instance. Wikipedia can be a good starting point, but books and journal articles will often have the best information. \n",
    "- Study previous work. Solution write-ups from past Kaggle competiotions are a greeat resource.\n",
    "- Use data visualization. Visualization can reveal pathologies in the distribution of a feature or complicated relationships that colud be simplified. Be sure to visualize your dataset as you work through the feature engineering process.\n",
    "\n",
    "**2. Mathematical Transforms**\n",
    "\n",
    "Relationships among numerical features are often expressed through mathmatical formulas, which you'll frequently come across as part of your domain researhc. In pandas, you can apply arithmetic operations to columns just as if they were ordinary numbers. Data visualization can suggest transformations, often a \"reshaping\" of a feature trhough powers or logarithms.  \n",
    "\n",
    "**3. Counts**\n",
    "\n",
    "Features describing the presence or absence of something oftten come in sets, the set of risk factor for disease, say. You can aggregate such features by creating a count. These features will be binary( 1 for Present, 0 for Absent ) or boolen (True or False). In Python, booleans can be added up jus as if they were integers. \n",
    "\n",
    "In traffic accidents are several features indicating wheter some roadway object was near the accident. This will create a count of the total number of roadway features nearby using the sum method : \n",
    "\n",
    "You could also use a dataframe's built-in methods to create boolean values. In the Concrete dataset are the amounts of components in a concrete formulation. Many formulations lack one or more componets. This wil count how many components are in a formulation with the dataframe's built-in greater-than gt method : \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8193b54",
   "metadata": {},
   "source": [
    "# 2) Mathematical Tranform\n",
    "\n",
    "autos[\"displacement\"] = (\n",
    "    np.pi * ((0.5 * autos.bore) ** 2) * autos.stroke * autos.num_of_cylinders\n",
    ")\n",
    "\n",
    "# If the feature has 0.0 values, use np.log1p (log(1+x)) instead of np.log\n",
    "accidents[\"LogWindSpeed\"] = accidents.WindSpeed.apply(np.log1p)\n",
    "\n",
    "# Plot a comparison\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "sns.kdeplot(accidents.WindSpeed, shade=True, ax=axs[0])\n",
    "sns.kdeplot(accidents.LogWindSpeed, shade=True, ax=axs[1]);\n",
    "\n",
    "# 3) Counts\n",
    "\n",
    "roadway_features = [\"Amenity\", \"Bump\", \"Crossing\", \"GiveWay\",\n",
    "    \"Junction\", \"NoExit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\",\n",
    "    \"TrafficCalming\", \"TrafficSignal\"]\n",
    "accidents[\"RoadwayFeatures\"] = accidents[roadway_features].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515d202f",
   "metadata": {},
   "source": [
    "**4. Building-Up and Breaking-Down Features**\n",
    "\n",
    "Often you'll have complex strings that can usefully be broken into simpler pieces. Some common examples : \n",
    "- ID numbers : '123-456-789'\n",
    "- Phone numbers : '(999) 555-0123'\n",
    "- Stret address : '8241 Kaggle Ln., Goose City, NV'\n",
    "\n",
    "Features like these will often have soem kind of structure that you cna make use of. US phone numbers, for instance that tells you the location of the caller, As always, some research can pay off here.\n",
    "\n",
    "The str accesor lets you apply string method like split directly to columns. \n",
    "\n",
    "**5. Group Transforms**\n",
    "\n",
    "Finally we have group transforms, whic aggregate information across multiple rows grouped by some category. With a group transform you can create features like : \"the average income of a person's state of residence\", or \"the proportion of movies released on a weekday, by gnere.\" If you had discovered a categroy interaction, a group transform over that category could be something good to investigate. \n",
    "\n",
    "Using an aggregation fucntion, a group transform combines two features : a categorical features that provides the grouping and another feature whose value you wish to aggregate. For an \"average income by state\", you would choose State for grouping feature mean for the aggregation function and Income for the aggregated feature. To compute this in Pandas, we use the groupby and transform method : "
   ]
  },
  {
   "cell_type": "raw",
   "id": "28469bd1",
   "metadata": {},
   "source": [
    "customer[\"AverageIncome\"] = (\n",
    "    customer.groupby(\"State\")  # for each state\n",
    "    [\"Income\"]                 # select the income\n",
    "    .transform(\"mean\")         # and compute its mean\n",
    ")\n",
    "\n",
    "# Frequency of categorical value \n",
    "\n",
    "customer[\"StateFreq\"] = (\n",
    "    customer.groupby(\"State\")\n",
    "    [\"State\"]\n",
    "    .transform(\"count\")\n",
    "    / customer.State.count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaed76a",
   "metadata": {},
   "source": [
    "**6. Tips on Creating Features**\n",
    "\n",
    "- Linear models learn sums and differneces naturally, but can't learn anything more complex. \n",
    "\n",
    "- Ratios seem to be difficult for most model to learn. Ratio combinations often lead to some easy performance gains. \n",
    "\n",
    "- Linear models and neural nets generally do better with normalized features. Neural nets especially need features scaled to valud not too far from 0. Tree-based models (like random forest and XGBoost) can sometimes benefit from normalization, but usually much less so. \n",
    "\n",
    "- Tree models can learn to approximate almost any combination of features, but when a combination is expecially important they can still benefit from having it explicitly created, especially when data is limited. \n",
    "\n",
    "- Counts are especially helpful for tree models, since these models don't have a natural way of aggregating information across many features at once. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921d997f",
   "metadata": {},
   "source": [
    "# Clustering with K-Means\n",
    "\n",
    "This lesson and the next make use of what are known unsupervised learning algorithms. Unsupervised algorithms don't make use of a target; instead, thier purpose is to learn some property of the data, to represent the structure of the features in a certain way. In the context of feature engineering for predictions, you could think of an unsupervised algorightms as a \"feature discovery\" technique.\n",
    "\n",
    "Cluertering simply means the assigning of data points to group based upon how similar the points are to each other. A clustering algorithm makes \"birds of feature flock together\", so to speak. \n",
    "\n",
    "When used for feature engineering, we could attempt to discover groups of customers representing a market segment, for instance, or geographic areas that share similar weather patteerns. Adding a feature of clustering labels can help machine learning models untangle complicated relationships of space or proximity. \n",
    "\n",
    "\n",
    "**1. Cluster Labels as a Feature** \n",
    "\n",
    "Applied to a single real-valued feature, clustering acts like a traditional \"binning\" or \"discretization\" transform. On multiple features, it's like \"multi-dimensional binning\". \n",
    "\n",
    "\n",
    "It's important to remember that this Cluster feature is categorical. Here, it's shonw with a lbel encoding as a typical clustering algorithm would produce; depending on your model, a ohe-hot encoding may be more appropriate. \n",
    "\n",
    "\n",
    "The motivating idea of adding cluster labels is that the clusters will break up complicated relationships across features into simpler chunks. Our model can then just learn the simpler chunks one-by-one instead having to learn the complicated whole all at once. It's a \"divide and conquer\" strategy.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3949e1db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
