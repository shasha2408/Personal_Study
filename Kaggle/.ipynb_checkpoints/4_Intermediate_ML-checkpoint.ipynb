{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d08c4a17",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "If we have some background in machine learning and we'd like to learn how to quickly improve the quality of our models. In this course, we will accelerate our machine learning expertise by learning how to :   \n",
    "\n",
    "- tackle data types often found in real-world datasets(missing values, categorical variables).  \n",
    "- design pipleine to improtve the quality of our machine learning code.  \n",
    "- use advanced techniques for model validation(CV)   \n",
    "- build state-of-the-art that are widely used to win Kaggle competiotns(XGBoost).  \n",
    "- Avoid common and important data science mistakes(leakage).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad311b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress of Machine Learning \n",
    "\n",
    "# Preprocessing \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "X_full = pd.read_csv('../../KAGGLE/Kaggle_House_Price/train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('../../KAGGLE/Kaggle_House_Price/test.csv', index_col='Id')\n",
    "\n",
    "# Obtain target and predictors\n",
    "y = X_full.SalePrice\n",
    "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "X = X_full[features].copy()\n",
    "X_test = X_test_full[features].copy()\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)\n",
    "                                                      \n",
    "# Gridsearch      \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the models\n",
    "model_1 = RandomForestRegressor(n_estimators=50, random_state=0)\n",
    "model_2 = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model_3 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\n",
    "model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\n",
    "model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n",
    "\n",
    "models = [model_1, model_2, model_3, model_4, model_5]\n",
    "\n",
    "# Make best model \n",
    "\n",
    "best_model = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Generate test predictions\n",
    "preds_test = best_model.predict(X_test)\n",
    "\n",
    "# Save predictions in format used for competition scoring\n",
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'SalePrice': preds_test})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d46eab",
   "metadata": {},
   "source": [
    "# Submit your results\n",
    "\n",
    "Once you have successfully completed Step 2, you're ready to submit your results to the leaderboard!  First, you'll need to join the competition if you haven't already.  So open a new window by clicking on [this link](https://www.kaggle.com/c/home-data-for-ml-course).  Then click on the **Join Competition** button.  _(If you see a \"Submit Predictions\" button instead of a \"Join Competition\" button, you have already joined the competition, and don't need to do so again.)_\n",
    "\n",
    "Next, follow the instructions below:\n",
    "1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n",
    "2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n",
    "3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n",
    "4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n",
    "\n",
    "You have now successfully submitted to the competition!\n",
    "\n",
    "If you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa78942",
   "metadata": {},
   "source": [
    "# Missing values\n",
    "1. Three Approches  \n",
    "\n",
    "    1) A simple options : Drop Columns with Missing Values  \n",
    "    \n",
    "    The simplest option is to drop columns with missing values. Unless most values in the dropped columns are missing, the model loses access to a lot of information with this approach.  \n",
    "    As an extreme example, consider a dataset with 10,000 rows, where one important column is missing a single entry. This approach would rop the column entirely.\n",
    "    \n",
    "    2) A Better option : Imputation\n",
    "    \n",
    "    Imputation fills in the missing values with some number. For instance, we can fill in the mean value along each column. The imputed value won't be exactly right in most cases, but it usually leads to more accurate models than you would get from dropping the column entirely.\n",
    "    \n",
    "    3) An Extension to Imputation\n",
    "    \n",
    "    Imputation is the standard approach, and it usually works well. However, imputed values may be systematically above or below thier actual values. Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing. In this approach, we imputed the missing values, as before. And, additionally, for each column with missing entries in the original dataset, we add a new column that shows the location of the imputed entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24e8cdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../../KAGGLE/Kaggle_House_Price/train.csv')\n",
    "\n",
    "# Select target\n",
    "y = data.SalePrice\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "melb_predictors = data.drop(['SalePrice'], axis=1)\n",
    "X = melb_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54746b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop columns with missing values):\n",
      "19003.3198630137\n",
      "MAE from Approach 2 (Imputation):\n",
      "19243.57294520548\n",
      "MAE from Approach 3 (An Extension to Imputation):\n",
      "19349.617465753425\n"
     ]
    }
   ],
   "source": [
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "\n",
    "# Drop columns in training and validation data\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop columns with missing values):\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))\n",
    "\n",
    "                               \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "print(\"MAE from Approach 2 (Imputation):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))\n",
    "\n",
    "# Make copy to avoid changing original data (when imputing)\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns\n",
    "\n",
    "print(\"MAE from Approach 3 (An Extension to Imputation):\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c71ff",
   "metadata": {},
   "source": [
    "# Categorical Variables \n",
    "\n",
    "1. Introduction :   \n",
    "A categorical variables takes only a limited number of values. Consider a surve that asks how often you eat breakfast and provides four options : \"Never\", \"Rarely\", \"Most days\", or \"Every day\". In this case, the data is categorical, because responses fall into a fixed set of categories. If people responded to a survey about which brand of car they owned, the responses would fall into categorical like \"Honda\", \"Toyota\", and \"Ford\". In this case, te data is also categorical.  \n",
    "\n",
    "2. Three Approcaches :  \n",
    "\n",
    "    1) Drop Categorical Variables :  \n",
    "    The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information. \n",
    "    \n",
    "    2) Ordinal Encoding :  \n",
    "    Ordinal encoding assigns each unique value to a different integer. This approach assumes an ordering of the categories : \"Never\"(0) < \"Rarely\"(1) < \"Most days\"(2) < \"Every day\"(3). This assumption makes sense in this example, because there is an indisputable ranking to the categories. Not all categorical variables have a clear ordering in the values, but we refer to those that do as ordinal variables. For tree-based models, you can expect encoding to work well with ordinal variables. \n",
    "    \n",
    "    3) One-Hot encoding :  \n",
    "    One-hot encoding creates new columns indicating the presence of each possible value in the original data. To understand this, we'll work through an example. In the original dataset, \"Color\" is a categorical variable with three categories : \"Red\", \"Yellow\", and \"Green\". The corresponding one-hot encoding contains one column for each possible value, and one row for each row in the original dataset. Wherever the original value was \"Red\", we put a 1 in the \"Red\" column; If the original value was \"Yellow\", we put a 1 in the \"Yellow\" column, and so on. \n",
    "    \n",
    "    In contrast to ordinal encoding, one-hot encoding does not assume an ordering of the categories. Thus you can expect this pproach to work particulary well if there is no clear ordering in teh categoricla data. We refer to categorical variables without an instinc ranking as nominal variables. \n",
    "    \n",
    "    One-hot encoding generally does not perform well if the categorical variable takes on a large number of values generally won't use it for variables taking more than 15 different values. \n",
    "    \n",
    "<font color = 'red'> 순서형 자료의 경우(n > 10) : replace, 명목형 자료의 경우(n < 10) : get dummies </font>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9323254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../../KAGGLE/Kaggle_House_Price/train.csv')\n",
    "\n",
    "# Separate target from predictors \n",
    "\n",
    "y = data.SalePrice\n",
    "X = data.drop(['SalePrice'], axis = 1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# Drop columns with missing values (simplest approach)\n",
    "cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] \n",
    "X_train_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87916d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Condition1</th>\n",
       "      <th>Condition2</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>HouseStyle</th>\n",
       "      <th>RoofStyle</th>\n",
       "      <th>RoofMatl</th>\n",
       "      <th>ExterQual</th>\n",
       "      <th>ExterCond</th>\n",
       "      <th>Foundation</th>\n",
       "      <th>Heating</th>\n",
       "      <th>HeatingQC</th>\n",
       "      <th>CentralAir</th>\n",
       "      <th>KitchenQual</th>\n",
       "      <th>Functional</th>\n",
       "      <th>PavedDrive</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>LowQualFinSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>Hip</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>Ex</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Typ</td>\n",
       "      <td>Y</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>619</td>\n",
       "      <td>20</td>\n",
       "      <td>11694</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>1774</td>\n",
       "      <td>1822</td>\n",
       "      <td>1828</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1828</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>774</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>PosN</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>Hip</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>CBlock</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>N</td>\n",
       "      <td>TA</td>\n",
       "      <td>Typ</td>\n",
       "      <td>Y</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>871</td>\n",
       "      <td>20</td>\n",
       "      <td>6600</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1962</td>\n",
       "      <td>1962</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>894</td>\n",
       "      <td>894</td>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>308</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>BrkTil</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>TA</td>\n",
       "      <td>Typ</td>\n",
       "      <td>Y</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>93</td>\n",
       "      <td>30</td>\n",
       "      <td>13360</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1921</td>\n",
       "      <td>2006</td>\n",
       "      <td>713</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>876</td>\n",
       "      <td>964</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>964</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>432</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>CulDSac</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>Hip</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Typ</td>\n",
       "      <td>Y</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>818</td>\n",
       "      <td>20</td>\n",
       "      <td>13265</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2002</td>\n",
       "      <td>2002</td>\n",
       "      <td>1218</td>\n",
       "      <td>0</td>\n",
       "      <td>350</td>\n",
       "      <td>1568</td>\n",
       "      <td>1689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1689</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>857</td>\n",
       "      <td>150</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Typ</td>\n",
       "      <td>Y</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>303</td>\n",
       "      <td>20</td>\n",
       "      <td>13704</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1541</td>\n",
       "      <td>1541</td>\n",
       "      <td>1541</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1541</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>843</td>\n",
       "      <td>468</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MSZoning Street LotShape LandContour Utilities LotConfig LandSlope  \\\n",
       "618       RL   Pave      Reg         Lvl    AllPub    Inside       Gtl   \n",
       "870       RL   Pave      Reg         Lvl    AllPub    Inside       Gtl   \n",
       "92        RL   Pave      IR1         HLS    AllPub    Inside       Gtl   \n",
       "817       RL   Pave      IR1         Lvl    AllPub   CulDSac       Gtl   \n",
       "302       RL   Pave      IR1         Lvl    AllPub    Corner       Gtl   \n",
       "\n",
       "    Condition1 Condition2 BldgType HouseStyle RoofStyle RoofMatl ExterQual  \\\n",
       "618       Norm       Norm     1Fam     1Story       Hip  CompShg        Ex   \n",
       "870       PosN       Norm     1Fam     1Story       Hip  CompShg        TA   \n",
       "92        Norm       Norm     1Fam     1Story     Gable  CompShg        TA   \n",
       "817       Norm       Norm     1Fam     1Story       Hip  CompShg        Gd   \n",
       "302       Norm       Norm     1Fam     1Story     Gable  CompShg        Gd   \n",
       "\n",
       "    ExterCond Foundation Heating HeatingQC CentralAir KitchenQual Functional  \\\n",
       "618        TA      PConc    GasA        Ex          Y          Gd        Typ   \n",
       "870        TA     CBlock    GasA        Gd          N          TA        Typ   \n",
       "92         Gd     BrkTil    GasA        Ex          Y          TA        Typ   \n",
       "817        TA      PConc    GasA        Ex          Y          Gd        Typ   \n",
       "302        TA      PConc    GasA        Ex          Y          Gd        Typ   \n",
       "\n",
       "    PavedDrive SaleType SaleCondition   Id  MSSubClass  LotArea  OverallQual  \\\n",
       "618          Y      New       Partial  619          20    11694            9   \n",
       "870          Y       WD        Normal  871          20     6600            5   \n",
       "92           Y       WD        Normal   93          30    13360            5   \n",
       "817          Y       WD        Normal  818          20    13265            8   \n",
       "302          Y       WD        Normal  303          20    13704            7   \n",
       "\n",
       "     OverallCond  YearBuilt  YearRemodAdd  BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  \\\n",
       "618            5       2007          2007          48           0       1774   \n",
       "870            5       1962          1962           0           0        894   \n",
       "92             7       1921          2006         713           0        163   \n",
       "817            5       2002          2002        1218           0        350   \n",
       "302            5       2001          2002           0           0       1541   \n",
       "\n",
       "     TotalBsmtSF  1stFlrSF  2ndFlrSF  LowQualFinSF  GrLivArea  BsmtFullBath  \\\n",
       "618         1822      1828         0             0       1828             0   \n",
       "870          894       894         0             0        894             0   \n",
       "92           876       964         0             0        964             1   \n",
       "817         1568      1689         0             0       1689             1   \n",
       "302         1541      1541         0             0       1541             0   \n",
       "\n",
       "     BsmtHalfBath  FullBath  HalfBath  BedroomAbvGr  KitchenAbvGr  \\\n",
       "618             0         2         0             3             1   \n",
       "870             0         1         0             2             1   \n",
       "92              0         1         0             2             1   \n",
       "817             0         2         0             3             1   \n",
       "302             0         2         0             3             1   \n",
       "\n",
       "     TotRmsAbvGrd  Fireplaces  GarageCars  GarageArea  WoodDeckSF  \\\n",
       "618             9           1           3         774           0   \n",
       "870             5           0           1         308           0   \n",
       "92              5           0           2         432           0   \n",
       "817             7           2           3         857         150   \n",
       "302             6           1           3         843         468   \n",
       "\n",
       "     OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  \\\n",
       "618          108              0          0          260         0        0   \n",
       "870            0              0          0            0         0        0   \n",
       "92             0             44          0            0         0        0   \n",
       "817           59              0          0            0         0        0   \n",
       "302           81              0          0            0         0        0   \n",
       "\n",
       "     MoSold  YrSold  \n",
       "618       7    2007  \n",
       "870       8    2009  \n",
       "92        8    2009  \n",
       "817       7    2008  \n",
       "302       1    2006  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be77660",
   "metadata": {},
   "source": [
    "Next, we obtain a list of all of the categorical variables in the training data.  \n",
    "\n",
    "We do this by checking the data type of each column. The object dtype indicates a column has text. For this dataset, the columns with text indicate categorical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed61ac47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']\n"
     ]
    }
   ],
   "source": [
    "# Get list of categorical variables\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "352cccfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop categorical variables):\n",
      "17952.591404109586\n"
     ]
    }
   ],
   "source": [
    "# Score from Approach 1 (Drop Categorical Variables)\n",
    "\n",
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97d32d1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories ['RRNn', 'RRAn'] in column 8 during transform",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-95e5320c2f95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mordinal_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrdinalEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlabel_X_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject_cols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mordinal_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mlabel_X_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject_cols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mordinal_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MAE from Approach 2 (Ordinal Encoding):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mTransformed\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \"\"\"\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mX_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0mX_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown, force_all_finite)\u001b[0m\n\u001b[1;32m    134\u001b[0m                     msg = (\"Found unknown categories {0} in column {1}\"\n\u001b[1;32m    135\u001b[0m                            \" during transform\".format(diff, i))\n\u001b[0;32m--> 136\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0;31m# Set the problematic rows to an acceptable value and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found unknown categories ['RRNn', 'RRAn'] in column 8 during transform"
     ]
    }
   ],
   "source": [
    "# Score from Approach 2 (Ordinary Encoding)\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Make copy to avoid changing original data \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Apply ordinal encoder to each column with categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n",
    "\n",
    "print(\"MAE from Approach 2 (Ordinal Encoding):\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61ac08b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (One-Hot Encoding):\n",
      "17514.224246575344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8363bd",
   "metadata": {},
   "source": [
    "# Pipelines \n",
    "\n",
    "1. Introduction :  \n",
    "Pipelines are a simple way to keep your data preprocessing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step. Many data scientists hack together models without pipelines, but piplines have some important benefits.\n",
    "\n",
    "    - Clearner code : Accounting for data at each step of preprocessing can get messy. Wiht a pipliner, you won't need to manually keep track of your training and validation data at each top. \n",
    "    - Fewer Bugs : There are fewer opportunities to misapply a step or forget a preprocessing step. \n",
    "    - Easier to Productionize : It can be surprisingly hard to transition a model from a prototype to something deployable at scale. We won't go into the many related concerns here, but piplines can help. \n",
    "    - More Options for Model Validation : You will see an example in the next tutorial, which cover cross-validation. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c299404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress of Machine Learning \n",
    "\n",
    "# Preprocessing \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "\n",
    "data = pd.read_csv('../../KAGGLE/Kaggle_House_Price/train.csv')\n",
    "\n",
    "# Separate target from predictors\n",
    "\n",
    "y = data.SalePrice\n",
    "X = data.drop(['SalePrice'], axis = 1)\n",
    "\n",
    "# Divide data into training and validation datasets\n",
    "\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "\n",
    "categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == 'object']\n",
    "\n",
    "# Select numerical columns\n",
    "\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "\n",
    "my_cols = categorical_cols + numerical_cols \n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bebae36",
   "metadata": {},
   "source": [
    "**Step 1 : Define Preprocesisng Steps**  \n",
    "Similar to how a pipeline bundles together preprocessing and modeling steps, we use the ColumnTransformer class to bundle together different preprocessing steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83ca0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "\n",
    "numerical_transformer = SimpleImputer(strategy = 'constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "\n",
    "categorical_transformer = Pipeline(steps = [\n",
    "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numercal and categorical data\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad693380",
   "metadata": {},
   "source": [
    "**Step2 : Define the Model**  \n",
    "Next, we define a random forest model with the familiar RandomForestRegressor class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adfa3092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators = 100, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c5282e",
   "metadata": {},
   "source": [
    "**Step3 : Create and Evaluate the Pipeline**  \n",
    "Finally, we use the Pipeline class to define a pipeline that bundles the prerprocessing and modeling steps. There are few important things to notice :   \n",
    "\n",
    "- With the pipeline, we preprocess the training data and fit the model in a single line of code. \n",
    "- With the pipeline, we supply the unprocessed features in X_valid to the predict command, and the pipeline automatically preprocesses the features before generating predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "019d61f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE :  17740.290308219177\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "\n",
    "my_pipeline = Pipeline(steps = [('preprocessor', preprocessor),\n",
    "                               ('model', model)])\n",
    "\n",
    "# Preprocessing of training data, fit model\n",
    "\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predcitions\n",
    "\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print(\"MAE : \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aa3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final process for prediction of test data \n",
    "\n",
    "preds_test = my_pipeline.predict(X_test)\n",
    "\n",
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'SalePrice': preds_test})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16702285",
   "metadata": {},
   "source": [
    "# Cross-Validation \n",
    "\n",
    "1. Introduction :  \n",
    "\n",
    "Machine learning is an iterative process.  \n",
    "\n",
    "You will face choices about what predictive variables to use, what types of models to use, what arguments to supply to those modles, etc. So far, you have made these choices in a data-driven way by measuring model quality with a validation set.   \n",
    "\n",
    "But there are some drawbacks to this approach. To see this, imagine you have a dataset with 5000 rows. You will typically keep about 20% of the data as a validation dataset, or 1000 rows. But this leaves some random chance in determining model score. That is, a model might do well on one set of 1000 rows, even if it would be inaccurate on a different 1000 rows.   \n",
    "\n",
    "At an extreme, you could imagine having only 1 row of data in the validation set. If you compare alternative models, which one makes the best predictions on a single data point will be mostly a matter of luck!  \n",
    "\n",
    "In general, the larget the validation set, the less randomness there is in our measure of model quality, and the more reliable it will be. Unfortunately, we can only get a large validation set by removing rows from our training data, and smaller training datasets mean worse models. \n",
    "\n",
    "2. What is cross-validation? :   \n",
    "\n",
    "In cross-validation, we run our modeling process on different subsets of the data to get multiple measures of model quality.   \n",
    "\n",
    "For example, we could begin by dividing the data into 5 pieces, each 20% of the full datset. In this case, we say that we have broken the data into 5 \"folds\". Then, we run one experiment for each fold\n",
    "\n",
    "| Experiment1 | Validation | Training | Training | Training | Training | \n",
    "| : --- : | : --- : | : --- : | : --- : | : --- : | : --- : | \n",
    "| Ex2 | | Validation | ||||\n",
    "| Ex3 | |  | Validation ||||\n",
    "| ... | | | ||||\n",
    "\n",
    "- In Experiment 1, we use the first fold as a validation set and everything else as training data. THis gives us a measure of model quality based on a 20% holdout set. \n",
    "- In Experiment 2, we hold out data from the second fold. The holdout set is then used to get ad second estimate of model quality. \n",
    "- We repeat this process, using every fold once as the holdout set. Putting this together, 100% of the daa is used as holdout at some point, and we end up with a measure of model quality that is based on all of the rows in the dataset.\n",
    "\n",
    "3. When should you use cross-validation ? :  \n",
    "\n",
    "Cross-validation gives a more accurate measure of model quality, which is especially important if you are making a lot of modeling decisions. However, it can take longer to run, because it estimates multiple models.  \n",
    "\n",
    "So, given these tradeoffs, when should you use each approach? \n",
    "\n",
    "- For small datasets, where extra computational burden isn't a big deal, you should run cross-validation\n",
    "- For larger datasets, a single validation set is sufficient. Your code will run faster, and you may have enough data that there's little need to re-use some of it for holdout. \n",
    "    \n",
    "There's no simple threshold for what consistutes a large vs. small dataset. But if your model takes a couple minutes or less to run, it's probably worth switching to cross-validation. Alternatively, you can run cross-validation and see if the scores for each expreiment seem close. If each experiment yields the same result, a single validation set is probably sufficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b7f2672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress of Machine Learning \n",
    "\n",
    "# Preprocessing \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "\n",
    "data = pd.read_csv('../../KAGGLE/Kaggle_House_Price/train.csv')\n",
    "\n",
    "# Separate target from predictors\n",
    "\n",
    "y = data.SalePrice\n",
    "X = data.drop(['SalePrice'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11bbeb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "\n",
    "numerical_transformer = SimpleImputer(strategy = 'constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "\n",
    "categorical_transformer = Pipeline(steps = [\n",
    "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numercal and categorical data\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Built pipeline \n",
    "\n",
    "my_pipeline = Pipeline(steps = [('preprocessor', preprocessor),\n",
    "                                ('model', RandomForestRegressor(n_estimators = 100, random_state = 0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fe07abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE scores : \n",
      " [17754.00712329 17653.89431507 17771.54866438 16398.27243151\n",
      " 19406.79760274]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates negative MAE\n",
    "\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y, cv = 5, scoring = 'neg_mean_absolute_error')\n",
    "\n",
    "print(\"MAE scores : \\n\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947689e9",
   "metadata": {},
   "source": [
    "The scoring parameter chooses a measure of model qaulity to report : in this case, we chose negative mean absolute error.   \n",
    "\n",
    "It is a little surpising that we specify negative MAE. Scikit-learn has a convetion where all metrics are difined so a high number is better. Using negatives here allows them to be consistent with that convention, though negative MAE is almost unheard of elsewhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1ba0a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE score (across experments) : \n",
      "17796.90402739726\n"
     ]
    }
   ],
   "source": [
    "print(\"Average MAE score (across experments) : \")\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990ad223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search with personal function using \n",
    "\n",
    "def get_score(n_estimators):\n",
    "    \"\"\"Return the average MAE over 3 CV folds of random forest model.\n",
    "    \n",
    "    Keyword argument:\n",
    "    n_estimators -- the number of trees in the forest\n",
    "    \"\"\"\n",
    "    # Replace this body with your own code\n",
    "    my_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', SimpleImputer()),\n",
    "    ('model', RandomForestRegressor(n_estimators= n_estimators, random_state=0))\n",
    "    ])\n",
    "    \n",
    "    scores = -1 * cross_val_score(my_pipeline, X, y, cv = 3, scoring = 'neg_mean_absolute_error')\n",
    "    return scores.mean()\n",
    "    \n",
    "results = {estimator : get_score(estimator) for estimator in np.arange(50, 450, 50)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab8104a",
   "metadata": {},
   "source": [
    "**최종 Roadmap**\n",
    "\n",
    "<font color = 'red'> \n",
    "    \n",
    "1. 데이터 셋 로딩   \n",
    "2. EDA   \n",
    "3. Visualization  \n",
    "3. Preprocessing   \n",
    "    - Missing values : Imputer vs 직접 작업 \n",
    "    - Categorical values : OneHotEncoder() vs pd.get_dummies() \n",
    "4. Make pipeline   \n",
    "    - preprocessing -> model 설정까지 한번에 \n",
    "5. Evaluation   \n",
    "    - cross-validation score\n",
    "    - GridSearchCV()를 통해 최적의 parameter 찾기 \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31de2b4",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e76660",
   "metadata": {},
   "source": [
    "1. Introduction :   \n",
    "\n",
    "For much of this course, you have made predictions with the random forest method, which achieves better performance than a single decision tree simply by averaging the predictions of many decision trees.  \n",
    "\n",
    "We refer to the random forest method as an 'ensemble method'. By definition, ensemble methods combine the predictions of several models.  \n",
    "\n",
    "2. What is ensemble ? \n",
    "\n",
    "http://www.dinnopartners.com/__trashed-4/\n",
    "\n",
    "\n",
    "3. Gradient Boosting :   \n",
    "\n",
    "Gradient boosting is a method that goes through cycles to iteratively add models into an ensemble.  \n",
    "\n",
    "It begins by initializing the ensemble with a single model, whose predictions can be pretty naive. (Even if its predictions are widly inaccurate, subsequent additions to the ensemble will address those errors.) \n",
    "\n",
    "Then, we start the cycle : \n",
    "\n",
    "- First, we use the current ensemble to generate predictions for each observation in the dataset. To make a prediction, we add the predictions from all models in the ensemble. \n",
    "- These predictions are used to calculate a loss function \n",
    "- Then, we use the loss function to fit a new model that will be added to the ensemble. Specifically, we determine model parameters so that adding this new model to the ensemble will reduce the loss. \n",
    "- Finally, we add the new model to ensemble, and ...\n",
    "- repeat them all!\n",
    "\n",
    "4. Example\n",
    "\n",
    "In this example, you'll work with the XGboost library. XGBoost stands for extreme gradient boosting, which is an implementation of gradient boosting with several additional features focused on performance and speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef7f176f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-da7ca952beee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "my_model = XGBRegressor()\n",
    "my_model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predictions = my_model.predict(X_valid)\n",
    "print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a963e1",
   "metadata": {},
   "source": [
    "**Parmeter Tuning**  \n",
    "XGBoost has a few parameters that can dramatically affect accuracy and training speed. The first parameters you should understand are : \n",
    "\n",
    "1. n_estimators :  \n",
    "\n",
    "n_estimators specifies how many times to go through the modeling cycle described above. It is equal to the number of modles that we include in the ensemble. \n",
    "- Too low a value causes underfitting, which leads to inaccurate predictions on both training data and test data.\n",
    "- Too high a value causes overfitting, which causes accurate predictions on training data, but inaccurate predictions on test data.\n",
    "\n",
    "2. early_stoppin_rounds :  \n",
    "\n",
    "early_stopping_rounds offers a way to automatically find the ideal value for n_estimators. Early stopping causes the model to stop iterating when the validation score stops imporiving, even if we aren't at the hard stop for n_estimators. It's smart to set a high value for n_estimators and then use early_stopping_Rounds to find the optimal time to stop iterting   \n",
    "\n",
    "Since random chance sometimes cuases a single round where validation scores don't imporve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. Setting early_stopping_rounds = 5 is a reasonable choice. In this case, we stop after 5 straight rounds of deteriorating validation score.\n",
    "\n",
    "3. learning_rate :   \n",
    "\n",
    "Instead of getting predictions by simply adding up the predictions from each component model, we can multiply the predictions from each model by a small number before adding them in.   \n",
    "\n",
    "This means each tree we add to the ensemble helps us less. So, we can set a higher value for n_estimators without overfitting. If we use early stopping, the appropriate4 number of trees will be determined automatically.  \n",
    "\n",
    "In general, a small learning rate an dlarge number of estimators will yield more accurate XGBoost models, thought it will also take the model longer to train since it does more iterations through the cycle. As default, XGBoost sets learning_rate = 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413c4fea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XGBRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-130433b43b82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m my_model.fit(X_train, y_train, \n\u001b[1;32m      3\u001b[0m              \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m              verbose=False)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XGBRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)], \n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41919e4",
   "metadata": {},
   "source": [
    "4. n_jobs :  \n",
    "\n",
    "On larget datasets where runtime is a consideration, you can use parallelism to build your model fatser. It's common to set the parameter n_jobs equal to the number of cores on your machine. On smaller datasets, this won't help.  \n",
    "\n",
    "The resulting model won't be any better, so micro-optimizing for fitting time is typically nothig but a distraction. But, it's useful in large datasets where you would otherwise spend a long time wating during the fit command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de4896f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XGBRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9a64f83a646b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m my_model.fit(X_train, y_train, \n\u001b[1;32m      3\u001b[0m              \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m              verbose=False)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XGBRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)], \n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0777f6f8",
   "metadata": {},
   "source": [
    "# Data Leakage \n",
    "\n",
    "1. Introduction :  \n",
    "\n",
    "Data leakage (or leakage) happens when your training data contains information about the target, but similar data will not be available when the model is used for prediction. This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production. \n",
    "\n",
    "In other words, leakage causes a model to look accurate until you start making decisions with the model, and then the models becomes very inaccurate. There are two main types of leakages : target leakage and train-test contamination.\n",
    "\n",
    "2. Target leakage :  \n",
    "\n",
    "Target leakage occurs when your predictors include data that will not be available at the time you make predictions. IT is important to think about target leakage in terms of the timing or chronocological order that data becomes available, not merely whether a feature helps make good predictions. \n",
    "\n",
    "An example will be helpful. Imagine you wnat to predict who wll get sick with pneumonia. THe top few rows of you raw data look like this :  \n",
    "\n",
    "| got_pneumonia | age | weight | male | took_antibiotic_medicine | ... | \n",
    "| : --- : | : --- : | : --- : | : --- : | : --- : | : --- : | \n",
    "| False | 65 | 100 | False | False | ... | \n",
    "| False | 72 | 130 | True | False | ... | \n",
    "| True | 58 | 100 |\tFalse |\tTrue | ... |\n",
    "\n",
    "People take anitibiotic medicinces after getting pneumonia in order to recover. THe raw data shows a strong relationship between those columns, but took_antibiotic_medicine is frequently changed after the valeu for got_pnumonia is determined.  \n",
    "\n",
    "The model would see that anyone who has a value of Flase for took_antibiotic_medicine didn't have pneumonia. Since avlidation data comes from the same source as training data, the pattern will repeat itself in validation, and the model will have great validation score.  \n",
    "\n",
    "But the model will be very inaccurate hwen subsequently deployed in the real world, because even patients who will get pneunomia won't have received antibiotics yet when we need to make predictions about their future help.  \n",
    "\n",
    "To preven this type of data leakage, any variabel updated after the target value is realized should be excluded.\n",
    "\n",
    "<font color = 'red'> 타겟 변수 이후에 영향을 주는 변수는 필요없음 </font>\n",
    "\n",
    "https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=tjdudwo93&logNo=221085844907\n",
    "\n",
    "3. Train-Test contamination :   \n",
    "\n",
    "이 data leakage는 학습 데이터와 validation 데이터를 제대로 구분하지 않았을 때 생긴다.\n",
    "\n",
    " \n",
    "\n",
    "Recall that validation는 모델이 이전에 고려되지 않았던 데이터에 대해 어떻게 작동하는지 측정하는 것을 말한다. Validation 데이터가 전처리에 영향을 준다면 이 과정에 손상이 올 수도 있다.\n",
    "\n",
    "만약  train_test_split()을 전처리 과정(missing value를 처리하는 imputer 같은) 이전에 한다고 생각해보자. 결과는? Validation 스코어는 좋겠지만 배포 후의 성능은 별로일 것이다.\n",
    "\n",
    " \n",
    "\n",
    "만약 validation 데이터가 train-test split을 기반으로 만들어졌을 때, validation 데이터를 모든 fitting에서 제외하고, 전처리 단계의 fitting에 포함시켜야 한다. Scikit-learn의 pipelines을 이용하면 더 쉽다. Cross-validation을 사용할 때는 파이프 라인 내에서 전처리를 수행하는 것이 훨씬 더 중요하다.\n",
    "\n",
    "<font color = 'red'> 데이터 전처리는 train-test split과정 이후 train에서만 적용되어야 함. Cross-validation의 경우에는 Pipeline을 생성후 전처리를 통해 수행하는게 정확함.\n",
    "</font>\n",
    "\n",
    "https://m.blog.naver.com/hongjg3229/221811766581"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57571ce",
   "metadata": {},
   "source": [
    "**최종 Roadmap**\n",
    "\n",
    "<font color = 'red'> \n",
    "    \n",
    "1. 데이터 셋 로딩   \n",
    "2. EDA   \n",
    "3. Visualization  \n",
    "3. Preprocessing   \n",
    "    - Pipeline을 통해 작업하지 않는경우 train_test_spliter를 통해 데이터를 분류하여 작업하는 것이 중요함. \n",
    "    - Missing values : Imputer vs 직접 작업 \n",
    "    - Categorical values : OneHotEncoder() vs pd.get_dummies() \n",
    "4. Make pipeline   \n",
    "    - preprocessing -> model 설정까지 한번에 \n",
    "5. Evaluation   \n",
    "    - cross-validation score\n",
    "    - GridSearchCV()를 통해 최적의 parameter 찾기 \n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "428.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
