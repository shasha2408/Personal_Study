{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2637a9b4",
   "metadata": {},
   "source": [
    "#  빅데이터의 이해 \n",
    "\n",
    "## 빅데이터 개요 및 활용 \n",
    "\n",
    "1. 빅데이터의 개념  \n",
    "테라바이트 이상의 정형 및 비정형 데이터. 가치를 추출하고 결과를 분석하는 기술\n",
    "\n",
    "\n",
    "2. DIKW 피라미드\n",
    "    - 데이터(Data) : 객관적 사실로서 다른 데이터와의 상관관계가 없는 가공하기 전의 순수한 수치\n",
    "    - 정보(Information) : 가공, 처리하여 데이터 간의 연관 관게와 함께 의미가 도출된 요소\n",
    "    - 지식(Knowledge) : 획득된 다양한 정보를 구조화하여 유의미한 정보를 분류하고 일반화시킨 결과\n",
    "    - 지혜(Wisdom) : 근본 원리에 대한 깊은 이해를 바탕으로 도출되는 창의적 아이디어\n",
    "\n",
    "\n",
    "3. 바이트의 크기 : KB-MB-GB-TB-PB-EB-ZB-YB\n",
    "\n",
    "\n",
    "4. 빅데이터의 특징 \n",
    "    - 규모(Volumne)\n",
    "    - 다양성(Variety)\n",
    "    - 속도(Velocity)\n",
    "    - 신뢰성(Veracity)\n",
    "    - 가치(Value)\n",
    "    - 정확성(Validity)\n",
    "    - 휘발성(Volatility)\n",
    "    \n",
    "    \n",
    "5. 데이터 지식경영의 종류\n",
    "    - 지식구분\n",
    "        - 암묵지(Private) : 개인에게 체화되어 있는 지식\n",
    "        - 형식지(Public) : 문서나 메뉴얼처럼 형상화된 지식\n",
    "    - 지식 상호작용\n",
    "        - 내면화 : 형식지 -> 암묵지\n",
    "        - 공동화 : 암묵지 -> 암묵지\n",
    "        - 표출화 : 암묵지 -> 형식지\n",
    "        - 연결화 : 형식지 -> 형식지 \n",
    "       \n",
    "       \n",
    "6. 빅데이터의 가치 : 경제적 자산, 불확실성 제거, 리스크 감소, 스마트한 경쟁력, 타 분야와의 융합\n",
    "\n",
    "\n",
    "7. 빅데이터의 가치 산정이 어려운 이요 \n",
    "    - 데이터 활용 방식의 다양화 : 언제/어디서/누가 활용할지 알 수 없어서 \n",
    "    - 새로운 가치 창출\n",
    "    - 분석기술의 급속한 발전 \n",
    "    \n",
    "    \n",
    "8. 빅데이터 위기 요인 및 통제 방안\n",
    "    - 위기요인 \n",
    "        - 사생활 침해 \n",
    "        - 책임 원칙 훼손 : 분석 대상이 되는 사람들이 예측 알고리즘의 희생양이 됨\n",
    "        - 데이터 오용\n",
    "    - 통제방안\n",
    "        - 사생활 침해 -> 책임의 강조\n",
    "        - 책임 원칙 훼손 -> 결과 기반의 책임 적용\n",
    "        - 데이터 오용 -> 알고리즘에 대한 접근 허용, 알고리즈미스트 전문가 필요\n",
    "        \n",
    "        \n",
    "9. 분석 가치 에스컬레이터 \n",
    "    - 묘사분석 : 과거에 어떤 일이 일어났고, 현재는 무슨 일이 일어나고 있는지 확인\n",
    "    - 진단분석 : 분석의 원인을 이해하는 과정\n",
    "    - 예측분석 : 무슨일이 일어날 것인지를 예측\n",
    "    - 처방분석 : 예측을 바탕으로 최적화하는 과정\n",
    "    \n",
    "---\n",
    "\n",
    "1. 빅데이터 업무 프로세스\n",
    "    - 도입단계 : 도입기획, 기술검토, 도입 조직 구성, 예산 확보\n",
    "    - 구축단계 : 요구사항 분석, 구현 테스트\n",
    "    - 운영단계 : 운영예산 고려\n",
    "    \n",
    "    \n",
    "2. 조직 구조 설계의 요소\n",
    "    - 업무활동 : 과업수행을 위해 수직업무 활동과 수평 업무 활동으로 구분\n",
    "        - 수직 업무 활동 : 경영계획, 예산 할당 등 우선순위를 결정\n",
    "        - 수평 업무 활동 : 업무 프로세스 절차별로 업무를 배분\n",
    "    - 부서화 : 조직구조 유형 설계\n",
    "        - 집중 구조 : 전사 분석 업무를 별도의 분석 전담 조직에서 수행\n",
    "        - 기능 구조 : 해당 부서에서 수행. 핵심 분석이 어려움.\n",
    "        - 분산 구조 : 분석 결과에 따른 신속한 피드백이 나오고, 베스트 프랙티스 공유가 가능함. \n",
    "    - 보고체계\n",
    "    \n",
    "\n",
    "3. 조직 구조 설계 특성\n",
    "    - 공식화\n",
    "    - 분업화\n",
    "    - 직무 전문화\n",
    "    - 통제 범위\n",
    "    - 의사소통 및 조정\n",
    "    \n",
    "    \n",
    "4. 데이터 사이언티스트 요구역량 \n",
    "    - 소프트 스킬 : 커뮤니케이션 능력, 논리적 비판능력, 스토리텔링 능력, 시각화\n",
    "    - 하드 스킬 : 분석기술의 숙련도, 빅데이터 관련 이론적 지식\n",
    "    \n",
    "    \n",
    "5. 데이터 거버넌스 개념  \n",
    "데이터 거버넌스는 기업에서 사용하는 데이터의 가용성, 유용성, 통합성, 보완성을 관리하기 위한 정책과 프로세스를 다루며 프라이버시, 보안성, 데이터 품질, 관리 규정 준수를 강조하는 모델\n",
    "\n",
    "\n",
    "6. 데이터 거버넌스 구성요소\n",
    "    - 원칙 : 지침과 가이드\n",
    "    - 조직 : DBA\n",
    "    - 프로세스\n",
    "    \n",
    "    \n",
    "7. 데이터 거버넌스 체계\n",
    "    - 데이터 표준화 : 용어설명, 명명규칙, 사전구축 \n",
    "    - 데이터 관리 체계 : 관리 원칙 수립\n",
    "    - 데이터 저장소 관리 : 전사 차원의 저장소 구성\n",
    "    - 표준화 활동 : 표준준수 여부를 주기적으로 점검 및 모니터링 \n",
    "    \n",
    "    \n",
    "8. 조직평가를 위한 성숙도 단계 \n",
    "    - 도입 단계 : 분석을 시작해 환경을 구축, 일부 조직에서 수행\n",
    "    - 활용 단계 : 전문 담당 부서에서 시행\n",
    "    - 확산 단계 : 전사 모든 부서에서 시행\n",
    "    - 최적화 단계 : 데이터 사이언스 그룹, 경영진 분석 활용 \n",
    "    \n",
    "    \n",
    "9. 개선 방안 수립\n",
    "    - 준비형 : 낮은 준비도 + 낮은 성숙도 \n",
    "    - 장착형 : 낮은 준비도 + 높은 성숙도, 정착이 필요한 기업\n",
    "    - 도입형 : 높은 준비도 + 낮은 성숙도, 도입이 필요한 기업 \n",
    "    - 확산형 : 높은 준비도 + 높은 성숙도 \n",
    "    \n",
    "---\n",
    "\n",
    "## 빅데이터 기술 및 제도 \n",
    "\n",
    "1. 빅데이터 플랫폼  \n",
    "빅데이터에서 가치를 추출하기 위해 일련의 과정(수집, 저장, 처리, 분석, 시각화)을 규격화한 기술이다.\n",
    "\n",
    "\n",
    "2. 빅데이터 플랫폼 구성요소\n",
    "    - 데이터 수집 : 정형/반정형/비정형 데이터 수집. ETL/크롤러/EAI\n",
    "    - 데이터 저장 : RDBMS, NoSQL\n",
    "    - 데이터 분석 : 텍스트 분석, 머신러닝, 통계, 데이터 마이닝\n",
    "    - 데이터 활용 : 데이터 가시화 및 BI, Open API연계, 히스토그램, 인포그래픽\n",
    "    \n",
    "    \n",
    "3. 하둡 에코시스템(하둡 프레임워크를 이루고 있는 다양한 서브 프로젝트들의 모임)\n",
    "    - 비정형 데이터 수집\n",
    "        - 척화(Chuckwa) : 분산된 서버에서 에이전트를 실행하고, 컬렉터가 데이터를 받아 HDFS에 저장 \n",
    "        - 플럼(Flume) : 로그 데이터를 수집, 이동하기 위해 이벤트와 에이전트를 활용하는 기술\n",
    "        - 스크라이브(Scribe) : 실시간으로 스트리밍 되는 로그데이터를 수집하여 분산 시스템에 데이터를 저장\n",
    "    - 정형 데이터 수집\n",
    "        - 스쿱(Sqoop) : SQL-to-Hadoop. RDBMS에서 HDFS로 데이터를 수집\n",
    "        - 히호(Hiho) : 깃허브에 공개되어 있음. Oracle, MySQL의 데이터만 전송 지원 \n",
    "    - 분산 데이터 저장(HDFS) : Hadoop Distributed File System의 약자. 네임노드(파일이름, 권한 등의 속성 기록 및 데이터 노드 위치 파악)와 데이터 노드(일정한 크기로 나눈 블록형태로 저장)로 구성됨.\n",
    "    - 분산 데이터 처리(Map Reduce) : 대용량 데이터 세트를 분산 병렬 컴퓨팅에서 처리하거나 생성하기 위한 목적. 모든 데이터를 Key-value 쌍으로 구성. Map-Shuffle-Reduce 순서대로 데이터를 처리함\n",
    "    - 분산 데이터베이스(Hbase)\n",
    "    - 리소스 관리(YARN)\n",
    "        - 리소스 매니저 : 스케줄러 역할을 수행하고, 클러스터 이용률 최적화를 수행\n",
    "        - 노드 매니저 : 노드 내의 자원 관리\n",
    "        - 애플리케이션 마스터 : 컨테이너를 실행\n",
    "        - 컨테이너 : 가상화 지원 \n",
    "    - 인메모리 처리(Aparch Spark) : 하둡 기반 대규모 데이터 분산처리시스템. 스트리밍 데이터, 온라인 머신러닝 등 실시간 데이터 처리 \n",
    "    - 데이터 가공\n",
    "        - 피그 : 대용량 데이터 집합을 분석하기 위한 플랫폼. SQL과 유사한 상태로 설계됨\n",
    "        - 하이브 : 하둡 기반의 DW 솔루션. HiveQL이라는 쿼리를 제공 \n",
    "    - 데이터 마이닝(Mahout) : 하둡기반으로 데이터 마이닝 알고리즘을 구현한 오픈 소스\n",
    "    - 실시간 SQL 질의(Impala) : 하둡기반의 실시간 SQL질의 시스템. 데이터 조화를 위한 HiveQL을 사용. Hbase와 연동이 가능 \n",
    "    - 워크플로우 관리(Oozie) : 하둡 작업을 관리하는 워크플로우 및 코디네이터 시스템\n",
    "    - 분산코디네이션(Zookeeper) : 분산 환경에서 서버들 간에 상호 조정이 필요한 다양한 서비스를 제공. 하나의 서버에만 서비스가 집중되지 않도록 서비스를 알맞게 분산하여 동시에 처리\n",
    "    \n",
    "\n",
    "4. 인공지능  \n",
    "인간의 지적능력을 인공적으로 구현하여 컴퓨터가 인간의 지능적인 행동과 사고를 모방할 수 있도록 하는 소프트 웨어 \n",
    "\n",
    "\n",
    "5. 개인정보보호  \n",
    "정보 주체(개인)의 개인정보 자기 결정권을 철저히 보장하는 활동\n",
    "\n",
    "\n",
    "6. 개인정보 보호의 필요성 : 유출시 피해 심각, 정보사회 핵심 인프라, 개인정보 자기 통제권\n",
    "\n",
    "\n",
    "7. 개인정보보호법 주요 내용\n",
    "    - 개인정보 수집-이용\n",
    "        - 정보 주체의 동의를 받은 경우 \n",
    "        - 법률에 특정한 규정이 있거나, 의무를 준수하기 위해 불가피한 경우\n",
    "        - 공공기관의 소관 업무 수행을 위해 불가피한 경우\n",
    "        - 정보 주체와의 계약 체결을 위하여 필요한 경우\n",
    "        - 정보 주체/범정대리인이 의사표시를 할 수 없는 경우\n",
    "        - 개인정보 처리자의 정당한 이익을 위하여 사용되는 경우\n",
    "        \n",
    "    - 개인정보 수집-이용 고지사항\n",
    "        - 목적\n",
    "        - 항목\n",
    "        - 이용기간\n",
    "        - 거부할 권리\n",
    "        \n",
    "    - 개인정보 제공\n",
    "        - 개인정보를 제공받는 자 \n",
    "        - 목적\n",
    "        - 항목\n",
    "        - 이용기간\n",
    "        - 거부할 권리\n",
    "    \n",
    "    - 개인정보 유출시 통지사항\n",
    "        - 개인정보 항목\n",
    "        - 시점과 그 경위\n",
    "        - 정보주체가 할 수 있는 방법\n",
    "        - 대응 조치 및 피해 구제절차 \n",
    "        - 담당부서 및 연락처\n",
    "        \n",
    "        \n",
    "8. 데이터 3법\n",
    "    - 개인정보보호법 : 데이터 이용 활성화를 위한 가명정보 도입. 개인정보 합리화 및 범위 명확화\n",
    "    - 정보통신망법 : '개인정보보호위원회' 변경\n",
    "    - 신용정보법 : 금융 분야 빅데이터 이용의 법적 근거. 마이 데이터 산업 도입. 개인정보보호 강화\n",
    "    \n",
    "    \n",
    "9. 가명정보 \n",
    "    - 개념 : 추가 정보의 사용 없이 특정 개인을 알아볼 수 없게 조치한 정보 \n",
    "    - 목적 및 대상 \n",
    "        - 통계작성 : 시장 조사 \n",
    "        - 과학적 연구\n",
    "        - 공익적 기록 보존\n",
    "        \n",
    "        \n",
    "10. 프라이버시 보호 모델\n",
    "    - k-익명성 : 주어진 데이터 집합에서 같은 값이 적어도 k개 이상 존재. 연결 공격 취약점 방어 \n",
    "    - l-다양성 : 비식별 되는 레코드들은 적어도 l개의 서로 다른 민감한 정보를 가져야 함. 동질성 공격, 배경지식에의한 공격 방어 \n",
    "    - t-근접성 : 특정 정보의 분포와 전체 데이터 집합에서 정보의 분포가 t이하의 차이를 보여야 함. 쏠림공격, 유사성 공격 방어 \n",
    "    - m-유일성 : 원본 데이터와 동일한 속성 값의 조합이 데이터에 최소 m개 이상 존재.\n",
    "    \n",
    "    \n",
    "11. 마이 데이터 : 개인은 데이터 주권인 자기 정보 결정권으로 개인 데이터의 활용과 관리에 대한 통제권을 개인이 가진다.\n",
    "\n",
    "---    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aff7d78",
   "metadata": {},
   "source": [
    "# 데이터 분석계획\n",
    "\n",
    "## 분석 방안 수립\n",
    "\n",
    "1. 분석 로드맵 단계 \n",
    "    - 데이터 분석 체계 도입 : 분석 과제 정의, 분석 기회 발굴, 로드맵 수립 \n",
    "    - 데이터 분석 유효성 검증 : 분석 알고리즘 설계, 분석 과제 파일럿 수행\n",
    "    - 데이터 분석 확산 및 고도화 : 시스템 구축, 유관 시스템 고도화 \n",
    "    \n",
    "    \n",
    "2. 분석 문제 정의\n",
    "    - 과제 : 처리해야할 문제 \n",
    "    - 분석 : 과제와 관련된 현상이나 원인, 해결 방안\n",
    "    - 문제 : 기대 상태와 현재 상태를 동일한 수준으로 맞추는 과정 \n",
    "    \n",
    "    \n",
    "3. 분석 문제 접근 절차 \n",
    "    - 하향식 접근 방식 : 분석 과제가 정해져 있는 경우\n",
    "        - 문제 탐색 : 비즈니스 모델 기반 문제 탐색, 분석 기회 발굴의 범위 확장 \n",
    "        - 문제 정의 \n",
    "        - 해결방안 탐색\n",
    "        - 타당섬 검토\n",
    "        - 선택\n",
    "    - 상향식 접근 방식 : 문제 정의 자체가 어려운 경우, 디자인 사고 접근법 이용 \n",
    "        - 특징 \n",
    "            - 비지도 학습 방법 : 데이터 자체의 결합, 연관성, 유사성 등을 중심으로 상태 분석 \n",
    "            - 프로토타이핑 접근법 사용 : 가설의 생성, 디자인에 대한 실험 \n",
    "        - 절차 \n",
    "            - 프로세스 분류\n",
    "            - 프로세스 흐름분석\n",
    "            - 분석 요건 식별\n",
    "            - 분석 요건 정의 \n",
    "            \n",
    "         \n",
    "4. 대상별 분석 기획 모형 \n",
    "    - 최적화 : 분석대상(Known) + 분석 방법(Known)\n",
    "    - 솔루션 : 분석대상(Known) + 분석 방법(Unknown)\n",
    "    - 통찰 : 분석대상(Unknown) + 분석 방법(Known)\n",
    "    - 발견 : 분석대상(Unknown) + 분석 방법(Unknown)\n",
    "    \n",
    "    \n",
    "5. 분석 마스터 플랜 수립\n",
    "    - 우선 순위 설정 \n",
    "        - 전략적 중요도\n",
    "        - 비즈니스 성과\n",
    "        - 실행 용이성 \n",
    "    - 로드맵 수립\n",
    "        - 업무 내재화 적용 수준\n",
    "        - 분석 데이터 적용 수준\n",
    "        - 기술 적용 수준 \n",
    "     \n",
    "    \n",
    "6. 분석 우선순위 평가\n",
    "    - 시급성 : 전략적 중요도에 부합하는지에 따른 시급성. 현재의 관점에 둘 것인지, 미래의 관점에 둘 것인지를 고려\n",
    "    - 난이도 : 현재 기업의 분석 수준과 데이터를 생성, 저장, 가공 분석하는 비용을 고려한 난이도\n",
    "    - Matrix\n",
    "        - 시급성(현재) + 난이도(쉬움) : 전략적 중요도 높음. 난이도가 쉽고 바로 적용 가능\n",
    "        - 시급성(현재) + 난이도(어려움) : 전략적 중요도 높음. 난이도가 어려움, 바로 적용 불가능\n",
    "        - 시급성(미래) + 난이도(쉬움) : 중장기적 관점 추진. 난이도 쉬움\n",
    "        - 시급성(미래) + 난이도(어려움) : 전략적 중요도가 낮으나 반드시 추진 되어야 함. \n",
    "        \n",
    "\n",
    "7. 데이터 분석 방법론의 분석 절차\n",
    "    - 분석 기획 : 비즈니스 이해 및 범위 설정. 프로젝트 정의 및 계획 수립. 프로젝트 위험계획 수립\n",
    "    - 데이터 준비 : 필요 데이터 정의. 데이터 스토어 설계. 데이터 수집 및 정합성 검증\n",
    "    - 데이터 분석 : 분석용 데이터 준비. 텍스트 분석, EDA, 모델링, 모델 평가 및 검증, 모델 적용 및 운영 방안 수립\n",
    "    - 시스템 구현 : 설계 및 구현\n",
    "    - 평가 및 전개 : 모델 발전 계획 수립, 프로젝트 평가 보고 \n",
    "\n",
    "\n",
    "8. KDD 분석 방법론\n",
    "    - 데이터 세트 선택 : 도메인에 대한 이해와 목표 데이터 구성 및 생성\n",
    "    - 데이터 전처리 : 노이즈, 결측치, 이상값 제거\n",
    "    - 데이터 변환 : 차원 축소, 변수 변환\n",
    "    - 데이터 마이닝 : 알고리즘 선택, 패턴 찾기, 분류, 예측\n",
    "    - 데이터 마이닝 결과\n",
    "    \n",
    "\n",
    "9. CRISP-DM 분석 방법론\n",
    "    - 업무 이해 : 비즈니스 이해, 목표 설정, 프로젝트 계획 수립\n",
    "    - 데이터 이해 : 데이터 수집 및 속성 이해, 초기 데이터 수집, 기술 분석, 탐색\n",
    "    - 데이터 준비 : 데이터 정제, 분리, 분석용 데이터 세트 선택\n",
    "    - 모델링 : 알고리즘 선택, 매개변수 최적화\n",
    "    - 평가 : 해석 결과가 프로젝트 목적에 부합하는지 평가\n",
    "    - 전개 : 완성된 모델을 업무에 적용하기 위한 계획 수립 \n",
    "\n",
    "\n",
    "10. SEMMA 분석 방법론\n",
    "    - 샘플링 : 분석 데이터 생성\n",
    "    - 탐색 : 기초 통계, 그래픽 탐색, 데이터 오류 검색, 비즈니스 이해\n",
    "    - 수정 : 데이터 수정/변환, 변수 생성, 선택, 변형\n",
    "    - 모델링 : 모델 구축, 패턴 ㅏㅂㄹ견, 알고리즘 적용\n",
    "    - 검증 : 모델 평가 검증\n",
    "    \n",
    "\n",
    "11. 빅데이터 분석 절차 \n",
    "    - 문제 인식\n",
    "    - 연구 조사 \n",
    "    - 모형화\n",
    "    - 자료 수집\n",
    "    - 자료 분석\n",
    "    - 분석 결과 공유 \n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518be99",
   "metadata": {},
   "source": [
    "# 데이터 수집 및 저장 계획 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5342fb96",
   "metadata": {},
   "source": [
    "## 데이터 수집 및 전환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df7a160",
   "metadata": {},
   "source": [
    "## 데이터 적재 및 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c51a4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
