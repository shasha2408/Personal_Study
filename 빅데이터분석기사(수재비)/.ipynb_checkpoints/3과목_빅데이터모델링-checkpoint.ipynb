{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d00c138",
   "metadata": {},
   "source": [
    "# 분석 모형 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b214a",
   "metadata": {},
   "source": [
    "## 분석 절차 수립 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda23df4",
   "metadata": {},
   "source": [
    "### 분석 모형 선정 \n",
    "\n",
    "1. 통계기반 분석 모형 선정 \n",
    "    \n",
    "    - 통계분석 : 불확실한 상황에서 객관적인 의사결정을 수행하기 위해 데이터를 수집하고, 처리, 분류, 분석 및 해석하는 일련의 체계\n",
    "    \n",
    "    - 기술통계 \n",
    "        - 수집된 데이터를 확률, 통계적으로 정리, 요약하는 기초적인 통계\n",
    "        - 데이터에 대한 대략적인 통계적 수치를 계산하고 도출\n",
    "        - 그래프를 활용하여 데이터를 파악\n",
    "        - 분석 초기 단계에서 데이터 분포의 특징 파악\n",
    "    \n",
    "    - 상관분석\n",
    "        - 두 개 이상의 변수 간에 존재하는 상호 연관성의 정도를 측정하여 분석하는 방법\n",
    "        - 단순상관 분석 : 두 변수 사이의 연관 관계 분석\n",
    "        - 다중상관 분석 : 셋 또는 그 이상의 변수들 사이의 연관 정도를 분석\n",
    "        - 변수 간의 상관 분석 : 데이터 속성에 따라서 수치적, 명목적, 순서적 데이터 등 을 가지는 변수 간의 상관분석\n",
    "    \n",
    "    - 회귀분석\n",
    "        - 하나 이상의 독립변수들의 종속변수에 미치는 영향을 추정할 수 있는 통계 기법\n",
    "        - 단순선형 회귀 : 독립변수가 1개, 종속변수와의 관계가 직선\n",
    "        - 다중선형 회귀 : 독립변수가 K개, 종속변수와의 관계가 선형\n",
    "        - 다항 회귀 : 독립변수와 종속변수와의 관계가 1차 함수 이상인 관계\n",
    "        - 곡선 회귀 : 독립변수가 1개이며 종속변수와의 관계가 직선\n",
    "        - 로지스틱 회귀 : 종속변수가 범주형인 경우 적용 \n",
    "        - 비선형 회귀 : 회귀식의 모양이 선형관계로 이뤄져 있지 않은 모형\n",
    "    \n",
    "    - 분산분석\n",
    "        - 두 개 이상의 집단 간 비교를 수행하고자 할 때 집단 내의 분선의 비교로 얻은 분포를 이용하여 가설검정을 수행\n",
    "        - 복수의 집단을 비교할 때 분산을 계산함으로써 집단 간에 통계적인 차이를 판정하는 분석 \n",
    "    \n",
    "    - 주성분분석\n",
    "        - 많은 변수의 분산 방식의 패턴을 간결하게 표현하는 주성분 변수를 원래 변수의 선형 결합으로 추출하는 통계 기법\n",
    "    \n",
    "    - 판별분석\n",
    "        - 집단에 대한 정보로부터 집단을 구별할 수 있는 판별 규칙을 만들고, 다변량 기법으로 조사된 집단에 대한 정보를 활용하여 새로운 개체가 어떤 집단인지를 탐색하는 통계기법\n",
    "        \n",
    "   \n",
    "2. 데이터 마이닝 기반 분석 모형 선정\n",
    "    \n",
    "    - 데이터 마이닝 : 대용량 데이터로부터 데이터 내에 존재하는 패턴, 관계 혹은 규칙 등을 탐색하고 통계적인 기법을 활용하여 모델화 하여 정보를 추출\n",
    "    \n",
    "    - 분류 모델 \n",
    "        - 범주형 변수 혹은 이산형 변수등의 범주를 예측하는 것\n",
    "        - 사전에 정해진 그룹이나 범주 중의 하나로 분류하는 모델 \n",
    "    \n",
    "    - 예측 모델\n",
    "        - 범주형 및 수치형 등의 과거 데이터로부터 특성을 분석하여 다른 데이터의 결과값을 예측하는 기법\n",
    "        - 회귀분석 : 관찰된 연속형 변수들에 대해 두 변수 사이의 모형을 구한 뒤 적합도를 측정해 내는 분석 기법\n",
    "        - 의사결정나무 : 의사결정 규칙을 트리구조로 도표화하여 분류와 예측을 수행하는 분성 방법\n",
    "        - 시계열 분석 : 연도별, 분기별, 월별 등 시계열로 관측되는 자료를 분석하여 미래를 예측\n",
    "        - 인공신경망\n",
    "    \n",
    "    - 군집화 모델 \n",
    "        - 이질적인 집단을 몇개의 동질적인 소집단으로 세분화 하는작업\n",
    "        - 군집들 사이의 관계를 분석하는 다변량 분석 기법\n",
    "        - 계층적 방법\n",
    "            - 병합적(응집분석) 방법 : 유사한 소집단들을 합쳐 새로운 소집단을 구성 \n",
    "            - 분할적(분할분석) 방법 : 전체 집단에서 유사성이 떨어지는 객체들을 분리하는 방법 \n",
    "        - 비 계층적 방법\n",
    "            - K-means Clustering\n",
    "    \n",
    "    - 연관규칙 모델 \n",
    "        - 데이터에 숨어 있으면서 동시에 발생하는 사건 혹은 항목 간의 규칙을 수치화하는 기법\n",
    "        - 장바구니 분석, 마케팅에서 활용된다\n",
    "        \n",
    "        \n",
    "3. 머신러닝 기반 분석 모형 선정\n",
    "    \n",
    "    - 지도 학습(Supervised Learning)\n",
    "        - 정답인 레이블(Label)이 포함되어 있는 학습 데이터를 통해 컴퓨터를 학습시키는 방법\n",
    "        - 설명변수와 목적변수 간의 관계성을 표현해내거나 미래 관측을 예측해내는 것 \n",
    "        - 로지스틱 회귀 : 반응변수가 범주형인 경우 적용되는 회귀분석 모형\n",
    "        - 인공신경망 분석 : 인간의 뉴런구조 모방\n",
    "        - 의사결정나무 \n",
    "        - 서포트 벡신 머신 : 데이터를 초평면 중에서 데이터들과 거리가 가장 먼 초 평면을 선택하여 분리하는 지도 학습\n",
    "        - 랜덤 포레스트 : 의사결정나무의 배깅과 부스팅보다 더 많은 무작위성을 주어 선형 결합\n",
    "        - 감성 분석 \n",
    "        \n",
    "    - 비지도 학습(Unsupervised Learning)\n",
    "        - 입력 데이터에 대한 정답인 레이블(Label)이 없는 상태에서 훈련 데이터를 통해 학습시키는 방법\n",
    "        - 현상의 설명이나 특징 도출, 패턴 도출 등의 문제에 사용\n",
    "        - 사전정보가 없는 상태에서 유용한 정보나 패턴을 탐색적으로 발견하고자 하는 데이터 마이닝의 성격과 유사 \n",
    "        - 군집화, 인공신경망, 딥러닝이 적용 \n",
    "        \n",
    "    - 강화 학습(Reinforcement Learning)\n",
    "        - 선택 가능한 행동 중 보상을 최대화하는 행동 혹은 행동 순서를 선택하는 학습 방법\n",
    "        - 행동에 대한 반응에 따라 보상이 주어진다\n",
    "        - 보상을 최대한 많이 얻도록 하는 행동을 유도하도록 학습을 진행한다\n",
    "        \n",
    "        \n",
    "4. 변수에 따른 분석 기법 선정 \n",
    "\n",
    "    - 변수의 개수에 따른 분석기법 \n",
    "        - 단일변수 분석(Univariate Analysis) : 연속형 변수는 히스토그램이나 박스플롯을 사용하여 평균, 최빈수, 중위수 등 과 함께 분포 확인\n",
    "        - 이변수 분석 : 변수의 유형에 따라 적절한 시각화 분석 방법 선택\n",
    "        - 다변수 분석 : 범주형 변수가 하나 이상 포함된 경우 변수를 범주에 따라 쪼갠 후, 단변수나 이변수 분석 방법에 따라 분석\n",
    "        \n",
    "    - 데이터 유형에 따른 분석 기법 \n",
    "    \n",
    "        | 독립변수/종속변수 | 연속형 변수 | 이산형/범주형 변수 | \n",
    "        | : --- : | : --- : | : --- : | \n",
    "        | 연속형 변수 | 회귀분석, 인공신경망 모델, K-최근접 이웃기법 | 로지스틱 회귀분석, 판별분석, K-최근접 이웃기법 | \n",
    "        | 이산형/범주형 변수 | 회귀분석, 인공신경망 모델, 의사결정나무(회귀) | 인공신경망 모델, 의사결정나무(분류), 로지스틱 회귀분석 |     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55662a4d",
   "metadata": {},
   "source": [
    "### 분석 모형 정의  \n",
    "\n",
    "1. 분석 모형 정의 : 분석 모형을 선정하고 모델에 적합한 변수를 선택하여 모형의 사양을 작성하는 기법\n",
    "\n",
    "\n",
    "2. 매개변수 \n",
    "    - 모델 내부에서 확인이 가능한 변수로 데이터를 통해서 산출이 가능한 값\n",
    "    - 모델에 의해 요구되어지는 값들\n",
    "    - 매개변수가 모델의 성능을 결정\n",
    "    - 가중치, 서포트 벡터, 결정계수\n",
    "    \n",
    "\n",
    "3. 초매개변수\n",
    "    - 모델에서 외적인 요소로 데이터 분석을 통해 얻어지는 값이 아닌 사용자가 직접 설정해주는 값\n",
    "    - 모델의 매개변수값을 측정하기 위해 알고리즘 구현과정에서 사용 \n",
    "    - 학습률(Learning Rate), 깊이(Depth), 은닉층의 개수, KNN에서 K의 개수 \n",
    "    \n",
    "    \n",
    "4. 분석 모형 정의 고려사항 \n",
    "    - 과소적합 : 적정 수준의 학습이 부족하여 실제 성능이 떨어지는 현상 \n",
    "    - 과대적합 : 학습 데이터에 대한 성능은 좋으나 실제 데이터에 성능이 떨어지는 현상 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ff2a3",
   "metadata": {},
   "source": [
    "### 분석 모형 구축 절차 \n",
    "\n",
    "1. 요건 정의 : 분석과제 정의를 통해 도출된 내용을 요건 정의로 구체화하는 과정 \n",
    "    - 분석요건 도출 \n",
    "        - 분석요건을 추출, 분석, 명세화하고 종합적으로 적합성을 검토\n",
    "        - 데이터 분석 업무의 배경, 주요 이슈, 기대효과, 제약사항을 사전에 정의함\n",
    "    \n",
    "    - 수행방안 설계 \n",
    "        - 탐색적 분석을 수행하여 분석 가능성을 검토\n",
    "        - 데이터베이스 접근 환경을 구축하고, 분석 대상 데이터의 존재 여부를 확인하는 등 기초 분석 수행 \n",
    "    \n",
    "    - 요건 확정 \n",
    "        - 수립된 기획안을 이해관계저와 공유하여 최종 요건을 확정\n",
    "       \n",
    "\n",
    "2. 모델링\n",
    "    - 모델링 마트 설계 및 구축\n",
    "        - 다양한 원천 데이터로부터 분석 대상 데이터 획득\n",
    "        - 탐색, 정제, 요약 등의 전처리르 통해 변수들을 식별\n",
    "        - 분석 대상 데이터를 적재해 모델 마트를 구축\n",
    "    \n",
    "    - 탐색적 분석과 유의 변수 도출\n",
    "        - 유의미한 변수를 파악하기 위해 목표값 별로 해당 변수의 분포된 값을 보고 차이가 큰지 파악\n",
    "        - 분석 모형 및 데이터의 유의성을 반복적으로 보정\n",
    "        - 최소한의 시간에 탐색적 분석을 완료하여 단위 분석에 대한 소요 시간을 추정\n",
    "     \n",
    "    - 모델링\n",
    "        - 업무 특성에 적합나 기법을 선택하거나 여러 모델링 기법을 결합해 적용 \n",
    "        - 시뮬레이션과 최적화를 결합해 적용 \n",
    "    \n",
    "    - 모델링 성능 평가 \n",
    "        - 정확도, 정밀도, 재현율, 향상도 등의 값으로 판단\n",
    "\n",
    "\n",
    "3. 검증 및 평가 \n",
    "    - 분석데이터를 훈련(60 ~ 80%)과 평가(20 ~ 40%) 데이터로 분리한 다음 검증 및 평가 \n",
    "    - 운영 상황에서 실제 테스트 \n",
    "        - 분석결과를 업무 프로세스에 가상으로 적용해 검증하는 실무 적용 직전의 활동 \n",
    "        - 테스트하기 위한 유사 운영환경을 구축\n",
    "        - 설계 절차에 따라 테스트하고 그 결과를 분석 \n",
    "    - 비즈니스 영향도 평가 : 투자 대비 효과 정량화 기법으로 비즈니스 영향도 평가\n",
    "\n",
    "\n",
    "4. 적용\n",
    "    - 운영 시스템에 적용과 자동화\n",
    "    - 주기적 리모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbbbd6c",
   "metadata": {},
   "source": [
    "## 분석 환경 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a013db",
   "metadata": {},
   "source": [
    "### 분석 도구 설정 \n",
    "\n",
    "1. R\n",
    "    - 통계 프로그래밍 언어인 S언어를 기반으로 만들어진 오픈 소스 프로그래밍\n",
    "    - 15,000개 이상의 패키지를 직접 추가하여 기능을 확장할 수 있음\n",
    "    - R Studio(IDE)\n",
    "    - Windows, Mac OS, Linux 등 다양한 OS 지원\n",
    "    \n",
    "    \n",
    "2. Python\n",
    "    - C언어 기반의 오픈 소스 프로그래밍 언어    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c871adc3",
   "metadata": {},
   "source": [
    "### 데이터 분할 \n",
    "\n",
    "\n",
    "- 훈련 데이터와 검증 데이터는 학습 과정에서 사용하며, 평가 데이터는 학습 과정에 사용되지 않고 평가를 위해 사용됨\n",
    "- 학습이 완료된 모형에 대하여 한 번도 사용하지 않은 평가데이터 활용\n",
    "- 훈련데이터 : 60 ~ 80% vs 평가데이터 : 20 ~ 40% 활용 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca0854",
   "metadata": {},
   "source": [
    "# 분석 기법 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf568254",
   "metadata": {},
   "source": [
    "## 분석 기법 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3262558",
   "metadata": {},
   "source": [
    "### 회귀 분석 \n",
    "\n",
    "1. 회귀분석 개념\n",
    "    - 독립변수와 종속변수 간에 선형적인 관계를 도출해서 종속변수를 예측하는 분석 기법\n",
    "    - 변수들 사이의 인과관계를 밝히고 모형을 적합(Fit)하여 관심 있는 변수를 예측하거나 추론하기 위한 분석 방법\n",
    "    \n",
    "\n",
    "2. 회귀 모형의 가정 \n",
    "    - 선형성 \n",
    "        - 독립변수와 종속변수가 선형적이어야 한다는 특성\n",
    "        - 독립변수의 변화에 따라 종속변수도 일정 크기로 변화\n",
    "        \n",
    "    - 독립성 \n",
    "        - 잔차와 독립변수의 값이 서로 독립적이어야 함\n",
    "        - 더빈-왓슨 검정을 통해 확인 가능\n",
    "        \n",
    "    - 등분산성\n",
    "        - 잔차의 분산이 독립변수와 무관하게 일정해야 함\n",
    "    \n",
    "    - 비상관성\n",
    "        - 관측치와 잔차는 서로 상관이 없어야 함\n",
    "        \n",
    "    - 정규성\n",
    "        - 잔차항이 정규분포의 형태를 이뤄야 함 \n",
    "        - Q-Q plot에서 직선의 형태를 띄어야 함\n",
    "        \n",
    "\n",
    "3. 회귀 모형 검증\n",
    "    - 회귀 모형이 통계적으로 유의미한가?\n",
    "        - F-통계량을 통해 확인\n",
    "        - 유의수준 5% 하에서 p-값이 0.05보다 작으면 통계적으로 유의미\n",
    "        \n",
    "    - 회귀계수들이 유의미한가?\n",
    "        - t-통계량을 통해 신뢰구간 확인\n",
    "        \n",
    "    - 회귀 모형이 얼마나 설명력을 갖는가? \n",
    "        - 회귀식 자체의 유의성을 확인\n",
    "        - 결정계수($R^2$)를 통해 판단\n",
    "        \n",
    "    - 회귀 모형이 데이터를 잘 적합하고 있는가?\n",
    "        - 잔차를 그래프로 그리고 회귀진단을 함\n",
    "        \n",
    "    - 데이터가 가정을 만족시키는가?\n",
    "        - 선형성, 독립성, 등분산성, 비상관성, 정규성 가정을 만족\n",
    "        \n",
    "        \n",
    "4. 단순 선형 회귀 분석\n",
    "    - 단순 선형 회귀식\n",
    "        - $Y = \\beta_0 + \\beta_1 X + \\epsilon$\n",
    "        - 회귀 모형 중에서 가장 단순한 모형\n",
    "        - 독립변수와 종속변수가 각각 한 개이며 오차항이 있는 선형관계로 이루어져 있음\n",
    "        \n",
    "    - 회귀계수 추정\n",
    "        - 최소제곱법을 사용하여 추정한다. \n",
    "        - 오차의 제곱 합이 최소가 되는 추세선이 가장 합리적인 추세선 \n",
    "        - $\\sum_{i=1}^{n}(y_i - (\\beta_0 + \\beta_1x_i))^2$\n",
    "        \n",
    "    - 회귀 분석의 검정\n",
    "        - 회귀 계수 검정 : 회귀계수 $\\beta_1$이 0이면 입력변수와 출력변수는 인과관계가 없음\n",
    "        - 결정계수 \n",
    "            - 결정계수($R^2$)는 전체 데이터를 회귀 모형이 얼마나 잘 설명하고 있는지를 보여주는 지표 \n",
    "            - $R^2 = \\frac{SSR}{SST} = \\frac{SSR}{SSR+SSE}$\n",
    "            - 전체 제곱합(SST) = $\\sum_{i=1}^{n}(y_i-\\overline{y})^2$\n",
    "            - 회귀 제곱합(SSR) = $\\sum_{i=1}^{n}(\\hat{y}-\\overline{y})^2$\n",
    "            - 오차 제곱합(SSE) = $\\sum_{i=1}^{n}(y_i-\\hat{y})^2$\n",
    "        - 회귀직선의 적합도 검토 : 결정계수를 통해 추정된 회귀식이 얼마나 타당한지 검토 \n",
    "        \n",
    "\n",
    "5. 다중 선형 회귀 분석 \n",
    "    - 다중 선형 회귀식\n",
    "        - $Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_k X_k + \\epsilon$\n",
    "        \n",
    "    - 모형의 통계적 유의성\n",
    "        - 통계적 유의성은 F-통계량으로 확인한다. \n",
    "        - n은 표본의 개수, k는 변수의 개수\n",
    "        - 회귀 제곱 평균(MSR) = $MSR = \\frac{SSR}{k}$\n",
    "        - 잔차 제곱 평균(MSE) = $MSE = \\frac{SSE}{n-k-1}$\n",
    "        - 총 제곱 평균(MST) = $MST = \\frac{SST}{n-1}$\n",
    "        - $F = \\frac{MSR}{MSE}$\n",
    "        \n",
    "    - 회귀 분석의 검정 \n",
    "        - 회귀 계수 유의성 : 회귀 계수 유의성 검토와 동일하게 t-통계량을 확인\n",
    "        - 결정계수($R^2$) : 전체 데이터를 회귀 모형이 얼마나 잘 설명하고 있는 지를 보여주는 지표\n",
    "        - 수정된 결정계수($Adjusted R^2$) \n",
    "            - 결정계수는 독립변수의 수에 따라 증가하는 성질이 있음\n",
    "            - 이를 방지하기 위해 수정된 결정계수를 사용\n",
    "            - $R_a^2 = 1-(n-1)\\frac{MSE}{SST}$\n",
    "        - 모형의 적합성 : 잔차와 종속변수의 산점도로 확인\n",
    "        - 다중공선성 \n",
    "            - 설명변수들 사이에 선형 관계가 존재하면 회귀계수의 정확한 추정이 난해함\n",
    "            - 분산팽창요인(VIF), 상태지수를 통해 검사를 진행\n",
    "            - 다중공선성 문제가 발생하면 문제가 있는 변수를 제거하거나 주성분회귀, 능형 회귀를 적용하여 문제를 해결\n",
    "    \n",
    "    - 변수선택 방법\n",
    "        - 전진 선택법 : 상수 모형부터 시작해 중요하다고 생각되는 설명변수를 차례로 모형에 추가하는 방식\n",
    "        - 후진 소거법 : 독립ㅂ변수 후보 모두를 포함한 모형에서 출발해 제곱합을 기준으로 가장 적은 영향을 주는 변수부터 하나씩 제거하여 변수를 선택\n",
    "        - 단게적 방법 : 새롭게 추가된 변수에 기인해 기존 변수가 그 중요도가 악화될 때 해당 변수의 제거를 검토함\n",
    "        \n",
    "    - 벌점화된 선택기준 : 모형의 복잡도에 벌점을 주는 방식\n",
    "        - AIC(Akaike information Criterion)\n",
    "            - 실제 데이터의 분포와 모형이 예측하는 분포 사이의 차이를 나타내는 방법\n",
    "            - $AIC = -2ln(L) + 2p$ \n",
    "            - $-2ln(L)$ : 모형의 적합도, $2p$ : 매개변수 계수\n",
    "            - AIC값이 높다는 것은 모형의 적합도가 높다는 것을 의미\n",
    "            - 독립변수가 많은 모형, 즉 p가 증가할수록 AIC값이 증가하게 되므로 좋지 않은 모형이다\n",
    "        - BIC(Bayesian information Criterion)\n",
    "            - AIC의 단점은 표본이 클수록 부정확해진다 \n",
    "            - $BIC = -2ln(L) + pln(n)$\n",
    "            - AIC의 벌점은 표본 크기에 상관없이 일정하지만, BIC의 벌점은 표본 크기가 커질수록 커진다\n",
    "            - 표본의 크기가 커질수록 복잡한 모형을 더 강하게 처벌한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4748e578",
   "metadata": {},
   "source": [
    "### 로지스트 회귀 분석 \n",
    "\n",
    "1. 로지스틱 회귀 분석 \n",
    "    - 독립변수가 수치형이고 반응변수(종속변수)가 범주형(이산형)인 경우 적용되는 회귀 분석 모형 \n",
    "    - 반응변수의 각 범주에 속할 확률이 얼마인지를 추정하여 기준치에 따라 분류하는 모형 \n",
    "    - $ Y = \\frac{exp(\\beta_0 + \\beta_1 X_1 + ... \\beta_k X_k}{1 + exp(\\beta_0 + \\beta_1 X_1 + ... \\beta_k X_k}$\n",
    "    \n",
    "2. 로지스틱 회귀 분석의 원리 \n",
    "\n",
    "- 원리 : 독립변수가 어느 숫자이든 상관없이 종속변수의 결과값이 항상 범위 [0,1] 사이에 있도록 한다\n",
    "\n",
    "- 오즈(Odds) \n",
    "    - 특정 사건이 발생할 확률과 그 사건이 발생하지 않을 확률의 비 \n",
    "    - $Odds(p) = \\frac{p}{1-p}$\n",
    "    \n",
    "- 로짓(Logit) 변환\n",
    "    - 오즈에 로그를 취한 함수로서 입력값의 범위가 [0,1]일 때 범위를 $(-\\infty ~ +\\infty)로 조정한다$\n",
    "    - $Logit(p) = log\\frac{p}{1-p} = log odds(p)$\n",
    "   \n",
    "- 시그모이드(Sigmoid) 함수\n",
    "    - 로짓함수에 역함수를 취하면 x의 값이 $(-\\infty ~ +\\infty)$일 때, y 값은 [0, 1]의 값을 가지게 된다 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f80d8",
   "metadata": {},
   "source": [
    "### 의사 결정 나무 \n",
    "\n",
    "1. 의사결정나무(Decision Tree) \n",
    "    - 의사결정 규칙을 나무(Tree)구조로 나타내어 전체 자료를 몇 개의 소집단으로 분류하거나 예측하는 분석\n",
    "    - 분석의 대상을 분류함수를 사용하여 의사결정 규칙으로 이루어진 나무 모형을 만듬\n",
    "    - 의사결정 문제를 시각화해서 의사결정이 이루어지는 시점과 성과 파악을 쉽게 해준다 \n",
    "    \n",
    "    \n",
    "2. 의사결정나무의 구성요소\n",
    "    - 부모마디(Parent Node) : 주어진 마디의 상위에 있는 마디 \n",
    "    - 자식마디(Child Node) : 하나의 마디로부터 분리되어 나간 2개 이상의 마디들\n",
    "    - 뿌리마디(Root Node) : 시작되는 마디로 전체 자료를 포함\n",
    "    - 끝마디(Terminal Node) : 잎(Leaft)노드, 자식마디가 없는 마디 \n",
    "    - 중간마디(Internal Node) : 부모 마디와 자식 마디가 모두 있는 마디 \n",
    "    - 가치(Branch) : 뿌리 마디로부터 끝마디까지 연결된 마디들 \n",
    "    - 깊이(Depth) : 뿌리 마디로부터 끝마디까지 중간 마디들의 수 \n",
    "    \n",
    "    \n",
    "3. 의사결정나무 분석 과정 \n",
    "    - 의사결정나무 성장 : 종속변수와 관계가 있는 독립변수를 추가하고, 분석의 목적과 자료 구조에 따라서 적절한 분리규칙을 찾아 나무를 성장시키는 과정으로, 정지 규칙을 충족시키면 중단\n",
    "    - 가지치기 : 분리 오류를 크게 할 위험이 높거나, 부적절한 추론 규칙을 가지고 있는 가지(Branch) 또는 불필요한 가지를 제거하는 단계 \n",
    "    - 타당성 평가 : 이익 도표, 위험 도표 또는 평가 데이터를 이용하여 교차 타당성을 이용한 평가 수행 단계 \n",
    "    - 해석 및 예측 : 구축된 의사결정나무를 해석하고 데이터의 분류 및 예측에 활용하는 단계 \n",
    "    \n",
    "    \n",
    "4. 의사결정나무의 성장 \n",
    "\n",
    "    - 분류 규칙(Splitting Rule) \n",
    "        - 분리변수(Split Variable)가 연속형인 경우, $ A = x_i \\leq s $\n",
    "        - 분리변수가 범주형인 경우, $ A = [1, 2, 3], A^c = [4] $\n",
    "        - 불순도 감소량을 가장 크게 하는 분할\n",
    "        - 최적 분리 기준에 의한 분할을 찾은 후 각 분할에 대해서도 동일한 과정 반복 \n",
    "\n",
    "    - 분리 기준(Splitting Criterion) \n",
    "        - 목표변수의 분포를 구별하는 정도를 순수도, 불순도에 의해서 측정한다\n",
    "        - 부모 마디의 순수도에 비해 자식 마디들의 순수도가 증가하도록 형성 \n",
    "        - 이산형 종속변수에 사용되는 분리 기준\n",
    "            - 카이제곱 통계량의 p-value : p-value가 가장 작은 예측변수의 최적 분리를 통해 자식 마디 형성\n",
    "            - 지니 지수(Gini index) : 지니 지수를 가장 감소시켜주는 예측변수의 최적 분리를 통해 형성 \n",
    "            - 엔트로피 지수(Entropy index) : 엔트로피 지수가 가장 낮은 예측변수의 최적 분리를 통해 형성 \n",
    "        - 연속형 종속변수에 사용되는 분리 기준\n",
    "            - F-통계량 : p-value 값이 가장 작은 예측변수의 최적 분리를 통해 형성\n",
    "            - 분산의 감소량 : 분산의 감소량을 최대화하는 기준의 최적 분리를 통해 형성\n",
    "\n",
    "    - 정지규칙\n",
    "        - 더 이상 분리가 일어나지 않고 현재의 마디가 끝마디가 되도록 하는 규칙\n",
    "        - 깊이(Depth), 레코드 수의 최소 개수를 지정 \n",
    "\n",
    "    - 가지 나무치기(Prunning)\n",
    "        - 너무 큰 나무 모형은 과대 적합, 너무 작은 나무 모형은 과속 적합 문제를 야기 \n",
    "        - 최적의 나무 크기를 비용-복잡도 가지치기를 활용하여 가지치기를 진행 \n",
    "    \n",
    "    \n",
    "5. 불순도 척도 \n",
    "\n",
    "    - 카이제곱 통계량 \n",
    "        - $ \\chi^2 = \\sum_{i=1}^{k}\\frac{(O_i - E_i)^2}{E_i}$\n",
    "        - 기대도수 = 열의 합게 X 합의 합계 / 전체 합계 \n",
    "\n",
    "    - 지니 지수\n",
    "        - $Gini(T) = 1 - \\sum_{i=1}^{k}P_{i}^2$\n",
    "        - 노드의 불순도를 나태나는 값 \n",
    "\n",
    "    - 엔트로피 지수 \n",
    "        - $Entropy(T) = -(\\sum_{i=1}^{k}P_{i}log_{2}P_i)$\n",
    "        - 무질서 정도에 대한 척도 \n",
    "\n",
    "\n",
    "6. 의사결정나무 알고리즘 \n",
    "    - CART(이진분할)\n",
    "        - 독립변수를 이분화하는 과정을 반복하여 이진트리 형태로 분류를 수행하는 알고리즘\n",
    "        - 가장 성취도가 좋은 변수 및 수준을 찾는 것에 중점\n",
    "        \n",
    "    - C4.5 or C5.0\n",
    "        - 가지치기를 사용할 때 학습자료를 사용하는 알고리즘\n",
    "        - 종속변수가 이산형이어야 함\n",
    "        - 엔트로피 지수를 사용 \n",
    "        - 각 마디에서 다지 분리가 가능하며 범주형 독립변수에 대해서는 범주 수만큼 분리가 일어남 \n",
    "        \n",
    "    - CHAD(다지분할)\n",
    "        - AID를 발전시킨 알고리즘\n",
    "        - 적당한 크기에서 성장을 중시하며 독립변수가 이산형이어야 함\n",
    "        - 카이제곱 통계량을 사용 \n",
    " \n",
    "    - QUEST\n",
    "        - 범주의 개수가 많은 범주형 변수로의 편향이 심각한 CADT의 문제점을 개선한 알고리즘\n",
    "        - 변수 선택 편향이 거의 없음\n",
    "        - 카이제곱 통계량을 사용\n",
    "        - 이진 분리를 사용 \n",
    "\n",
    "\n",
    "7. 의사결정나무 활용 \n",
    "    - 활용\n",
    "        - 분류 : 여러 예측변수들에 근거해서 관측 개체의 목표변수 범주를 몇 개의 등급으로 분류하고자 하는 경우에 활용\n",
    "        - 예측 : 자료에서 규칙을 찾아내고 미래의 사건을 예측함\n",
    "        - 차원축소 및 변수 선택 : 독립변수 중에서 종속변수에 큰 영향을 미치는 변수들을 구분할 때 사용 \n",
    "        - 교호작용 및 효과의 파악 : 독립변수들을 결합해서 종속변수에 작용하는 규칙을 파악하고자 하는 경우에 사용 \n",
    "        \n",
    "    - 장점\n",
    "         - 해석의 용이 \n",
    "         - 상호작용 효과의 해석가능\n",
    "         - 비모수적 모형\n",
    "         - 유연성과 정확도가 높음\n",
    "    - 단점\n",
    "        - 비연속성\n",
    "        - 선형성 또는 주 효과의 결여 \n",
    "        - 비안정성 \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247f0b2",
   "metadata": {},
   "source": [
    "### 인공신경망 \n",
    "\n",
    "1) 인공신경망\n",
    "\n",
    "-   사람 두뇌의 신경세포인 뉴런이 전기신호를 전달하는 모습을 모방한 기계학습의 모델\n",
    "-   입력값을 받아서 출력값을 만들기 위해 활성화 함수를 이용\n",
    "\n",
    "2) 인공신경망 역사\n",
    "\n",
    "-   1세대(1943~1986)\n",
    "    -   퍼셉트론이라는 선형 분류가 가능한 순방향 신경망을 제안\n",
    "    -   XOR 선형 분리 불가 문제 발생\n",
    "-   2세대(1986~2006)\n",
    "    -   다층 퍼셉트론과 역전파 알고리즘의 등장\n",
    "    -   은닉층을 통해 XOR문제 해결\n",
    "-   3세대(2006~)\n",
    "    -   과적합 문제 및 기울기 소실 문제 해결\n",
    "\n",
    "**순방향 신경망(Feed Forward Neural Network)**  \n",
    "입력데이터가 입력층 -> 은닉층 -> 출력층의 순서로 전파되어 판별함수 값으로 변환되는 신경망\n",
    "\n",
    "**다층 퍼셉트론(Multi-Layer Perceptrons)**  \n",
    "입력층과 출력층 사이에 하나 이상의 은닉층을 두어 비선형적으로 분리되는 데이터 대해 학습이 가능한 퍼셉트론\n",
    "\n",
    "**역전파 알고리즘(Back Propagation Algorithm)**  \n",
    "역방향으로 가중치 갱신을 통해 오차를 최소화키도록 학습시키는 알고리즘\n",
    "\n",
    "**은닉층(Hidden Layer)**  \n",
    "인공신경망에서 입력층과 출력층 사이에 위치하여 내부적으로 동작하는 계층\n",
    "\n",
    "**기울기 소실문제(Gradient Vanishing Problem)**  \n",
    "오차 역전파에서 계산 결과와 정답과의 오차를 통해 가중치를 수정하는데, 입력층으로 갈수록 기울기가 사라져 가중치들이 업데이트 되지 않아 최적의 모델을 찾을 수 없는 문제. 계층을 이동할 때마다 노드의 활성화 함수의 미분 값을 곱하게 되는데, 시그모이드 함수는 미분값이 0~0.25로 입력층에 갈수록 0에 가까워져 기울가 사라져 가중치가 적용되지 않음\n",
    "\n",
    "**활성화 함수(Activation Function)**  \n",
    "인공신경망 모델에서 입력 신호의 총합을 출력 신호로 변환하는 함수로, 입력 받은 신호를 얼마나 출력할지 결정하고, 다음 단계에서 출력된 신호의 활성화 여부를 결정하는 함수\n",
    "\n",
    "3) 인공신경망의 구조\n",
    "\n",
    "-   퍼셉트론\n",
    "    -   입력층, 출력층으로 구성한 인공신경망 모델\n",
    "    -   입력값, 가중치, 순 입력함수, 활성화 함수, 예측값(출력값)\n",
    "    -   $\\sum\\_{i=1}^{n}(w_j x_j) $\n",
    "    -   순 입력함수 값을 활성화 함수의 임곗값과 비교하여 예측값을 출력\n",
    "    -   XOR의 선형 분리 문제점 존재  \n",
    "        [##_Image|kage@TyAv0/btrtIfaNjQZ/Wfu8A8kLQ7GcPKWDk9jHzk/img.png|CDM|1.3|{\"originWidth\":523,\"originHeight\":297,\"style\":\"alignCenter\"}_##]\n",
    "-   다층 퍼셉트론\n",
    "    -   입력층과 출력층 사이에 하나 이상의 은닉층을 두어 비선형적으로 분리되는 데이터에 대해 학습이 가능한 퍼셉트론\n",
    "    -   역전파 알고리즘을 통해 다층으로 만들어진 퍼셉트론의 학습이 가능\n",
    "    -   활성화 함수로 시그모이드 함수 이용\n",
    "    -   문제점\n",
    "        -   과대 적합 : 학습 데이터 부족으로 인한 과적합 문제 발생\n",
    "        -   기울기 소실 : 시그이드 함수의 편미분은 진행할수록 0에 가까워지는 특성이 존재. RELU함수로 이를 해결\n",
    "-   다층 퍼셉트론 계산 과정(순전파)\n",
    "    -   은닉층이 1개이고, 입력 데이터의 개수는 4개($x\\_0$ 는 Bias Unit)\n",
    "\n",
    "$$a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3)$$  \n",
    "$$a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3)$$  \n",
    "$$a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3)$$  \n",
    "$$h_{\\Theta}(x) = a_1^{(3)} = g(\\Theta_{10}^{(2)}a_0 + \\Theta_{11}^{(2)}x_0 + \\Theta_{21}^{(2)}x_0 + \\Theta_{31}^{(2)}x_0)$$\n",
    "\n",
    "$$z^{(j+1)} = \\Theta^{(j)}a^{(j)}$$  \n",
    "$$h_{\\Theta}(x) = a^{(j+1)} = g(z^{(j+1)})$$\n",
    "\n",
    "4) 활성화 함수\n",
    "\n",
    "-   순 입력함수로부터 전달받은 값을 출력값으로 변환해 주는 함수\n",
    "-   계단함수 : 임곗값을 기준으로 활성화(y축 1) 또는 비활성화(y축 0)가 됨\n",
    "-   부호함수 : 임곗값을 기준으로 양의부호(+1) 또는 음의부호(-1)를 출력\n",
    "-   시그모이드 함수 : 로지스틱 회귀모형과 작동원리가 유사함, 기울기 소실의 원리\n",
    "-   tanh함수 : 하이퍼볼릭 탄젠트 함수\n",
    "-   ReLU(Recified Linear Unit) :\n",
    "    -   X값이 0보다 큰 경우 y값도 지속적으로 커짐\n",
    "    -   시그모이드 기울기 소실 문제를 해결\n",
    "    -   X가 0보다 적은 경우 기울기가 0이므로 뉴런이 죽을 수 있음\n",
    "-   Leaky ReLU : ReLU 함수의 뉴런이 죽는 현상을 방지함\n",
    "\n",
    "5) 인공신경망 학습\n",
    "\n",
    "-   순전파(Feed Forward Propagation)\n",
    "    -   입력층(Input Layer)에서 출력층(Output Layer)까지 정보가 전달되는 과정\n",
    "    -   입력층(Input Layer)에서 은닉층(Hidden Layer) 방향으로 이동하면서 각 입력값의 가중치를 곱한다\n",
    "    -   은닉층(Hidden Layer)에서는 가중치가 반영된 입력값의 합계를 활성화 함수로 계산하고 결괏값을 출력층(Output Layer)으로 전달\n",
    "-   손실 함수(Loss Function)\n",
    "    -   실젯값과 예측값의 차이를 비교하는 지표\n",
    "    -   평균 제곱 에러(MSE) : $MSE = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - h\\_{\\Theta}(x_i))^2$\n",
    "    -   교차 엔트로피 오차(CEE) : $E = - \\sum_{i=1}^{n}(t_i log y_i)$\n",
    "-   경사 하강법(Gradient Descent)\n",
    "    -   기울기를 낮은 쪽으로 계속 이동시켜 최적의 매개변수를 찾는 기법\n",
    "    -   함수의 기울기를 구하고 경사의 절댓값이 낮은 쪽으로 계속 이동시켜 극값에 이를 때까지 반복시키는 기법\n",
    "    -   학습률(Learning Rate)는 갱신하는 양으로 초 매개변수임\n",
    "    -   경사 하강법은 좋은 학습률을 설정하지 않으면 전역 최소값(Global Maximum)이 아닌 지역 최소값(Local Maximum)에 수렴할 수 있음\n",
    "-   오차 역전파(Back Propagation)\n",
    "    -   계산 결과와 정답의 오차를 구하고 오차와 관련된 값들의 가중치를 수정하여 오차가 작아지는 방향으로 일정 횟수를 반복해서 수정하는 방법\n",
    "    -   오차 역전파 계산 방법 : [오차 역전파 계산](https://blog.naver.com/wjdtjrrb05/222262194193)\n",
    "\n",
    "6) 인공신경망 학습 절차\n",
    "\n",
    "-   미니 배치 학습\n",
    "    -   훈련 데이터 일부를 무작위로 추출하는 과정\n",
    "    -   미니 배치의 손실 함수(Loss Function)을 줄이는 것이 목표\n",
    "-   기울기 산출\n",
    "    -   미니배치의 손실 함수값을 줄이기 위해 각 가중치 매개변수의 기울기를 구하는 과정\n",
    "    -   순전파 -> 역전파 과정을 통해 기울기를 계산\n",
    "-   매개변수 갱신\n",
    "    -   가중치 매개변수를 기울기 방향으로 조금씩 갱신하는 과정\n",
    "    -   경사하강법 적용\n",
    "-   반복\n",
    "    -   최적값을 찾을 때까지 1~3 반복 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa424cdd",
   "metadata": {},
   "source": [
    "### 서포트 벡신 머신 \n",
    "\n",
    "1) 서포트 벡터 머신(Support Vector Machine; SVM)\n",
    "\n",
    "- 벡터 공간에서 학습 데이터 속한 2개의 그룹을 분류하는 선형 분리자를 찾는 기하학적 모델\n",
    "- 데이터를 분리하는 초평면(Hyperplane) 중에서 데이터들과 거리가 가장 먼 초평면을 선택하여 분리하는 지도 학습 기반의 이진 선형 분류 모델\n",
    "- 최대 마진을 가지는 비확률적 선형 판별 분석에 기초한 이진 분류기\n",
    "- 변수 속성간의 의존성은 고려하지 않으며(다중공선성 등) 모든 속성을 활용하는 기법이다 \n",
    "- 훈련 시간은 상대적으로 느리나, 정확성이 뛰어나며 다른 방법보다 과대 적합의 가능성이 낮은 모델 \n",
    "\n",
    "2) 서포트 벡터 머신 구성요소\n",
    "\n",
    "- 결정 경계(Decision Boundary) : 데이터 분류의 기준이 되는 경계 \n",
    "- 초평면(Hyperplane) : n 차원 공간의 n-1 차원 평면\n",
    "- 마진(Margin) : 결정 경계에서 서포트 벡터까지의 거리, 마진을 최대화 -> 최대 마진 \n",
    "- 서포트 벡터 : 학습 데이터 중에서 결정 경계와 가장 가까이에 있는 데이터들의 집합\n",
    "- 슬랙 변수(Slack Variables) : 완벽한 분리가 불가능할 때 선형적으로 분류를 위해 허용된 오차를 위한 변수(Soft Margin SVM)\n",
    "\n",
    "3) 서포트 벡터 머신 종류 \n",
    "\n",
    "- 하드 마진 SVM \n",
    "\t- 마진의 안쪽이나 바깥쪽에 절대로 잘못 분류된 오 분류를 허용하지 않는 SVM\n",
    "    - 노이즈로 인하여 최적의 결정 경계를 잘 못 구하거나 못 찾을 수 도 있음\n",
    "- 소프트 마진 SVM\n",
    "\t- 마진의 안쪽이나 바깥쪽에 절대로 잘못 분류된 오 분류를 허용하는 SVM\n",
    "    - 어느 정도 오류를 허용함으로써 과대 적합 방지 \n",
    "    \n",
    "4) 서포트 벡터 머신 적용 기준\n",
    "\n",
    "- 선형으로 분리 가능한 SVM\n",
    "\t- 최적의 결정 경계를 기준으로 1, -1로 구분하여 분류 모형으로 사용 \n",
    "- 선형으로 분리 불가능한 SVM\n",
    "\t- 저차원 공간을 고차원 공간으로 매핑할 경우에 발생하는 연산의 복잡성은 커널 트릭을 통하여 해결이 가능\n",
    "\n",
    "\n",
    "**커널 트릭(Kernel Trick)**  \n",
    "저차원에서 함수의 계산만으로 원하는 풀이가 가능한 커널 함수를 이용하여 고차원 공간으로 매핑할 경우에 증가하는 연산량의 문제를 해결하는 기법. \n",
    "2차원에서 분류할 수 없는 문제를 3차원으로 매핑하여 선형 분류하게 되는데, 내적 함수 k를 통해 각각의 매핑함수를 정의하지 않고 내적 함수만 정의함으로써 서포트 벡터 머신의 계산량을 줄일 수 있다. 맵핑 공간에서의 내적과 동등한 함수를 커널함수라고 한다. \n",
    "\n",
    "5) 커널 함수의 종류\n",
    "\n",
    "- 선형 커널 : 기본 유형의 커널, 1차원이고 계산이 빠름\n",
    "- 다항 커널 : 선형커널의 일반화된 공식, 효과성과 정확도 측면에서 떨어짐\n",
    "- 가우시안 커널 : 일반적으로 사용하며, 데이터에 대한 사전 지식이 없는 경우 활용\n",
    "- 가우시안 RBF : 비선형 데이터가 있는 경우에 일반적으로 활용됨\n",
    "- 시그모이드 커널 : 인공신경망에서 선호되는 커널 \n",
    "\n",
    "6) 서포트 벡터 머신 장/단점 \n",
    "\n",
    "- 장점\n",
    "\t- 서포트 벡터만을 사용하여 결정 경계를 생성하므로 데이터가 적을 때 효과적\n",
    "    - 전체 데이터 포인트와의 거리를 계산하지 않고 서포트 벡터와의 거리만을 계산하면 되기 때문에 연산량 최소화\n",
    "    - 정확성이 뛰어나며, 커널 트릭을 활용하여 비선형 모델 분류 가능 \n",
    "    - 다른 모형보다 과대 적합의 가능성이 낮고, 노이즈의 영향이 적음\n",
    "    \n",
    "- 단점 \n",
    "\t- 데이터 전처리 과정이 중요 \n",
    "    - 데이터 세트의 크기가 클 경우 모델링에 많은 시간이 소요됨\n",
    "    - 커널과 모델의 매개변수를 조절하기 위해 많은 테스트가 필요 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa645607",
   "metadata": {},
   "source": [
    "### 연관성 분석 \n",
    "\n",
    "1) 연관성 분석\n",
    "\n",
    "-   데이터 내부에 존재하는 항목 간의 상호 관계 혹은 종속 관계를 찾아내는 분석 기법\n",
    "-   데이터 간의 관계에서 조건과 반응을 연결하는 분석으로 장바구니 분석, 서열 분석 이라고도 함\n",
    "\n",
    "2) 연관성 분석 특징\n",
    "\n",
    "-   목적변수가 없어 분석 방향이나 목적이 없어도 적용 가능\n",
    "-   조건 반응으로 표현되어 결과를 쉽게 이해할 수 있음\n",
    "-   적절한 세분화로 인한 품목 결정이 장점이지만 너무 세분화된 품목은 의미 없는 결과를 도출\n",
    "-   교차 판매, 묶음 판매, 상품 진열, 거래 후 쿠폰 제공, 온라인 쇼핑의 상품 추천\n",
    "\n",
    "3) 연관성 분석 추천 지표\n",
    "\n",
    "-   지지도\n",
    "    -   전체 거래 중 항목 A와 B를 동시에 포함하는 거래의 비율\n",
    "    -   $P(A \\cap B) = \\frac{A와 B가 동시에 포함된 거래 수}{전체 거래 수}$\n",
    "-   신뢰도\n",
    "    -   A 상품을 샀을 때 B 상품을 살 조건부 확률에 대한 척도\n",
    "    -   $P(B|A) = \\frac{A와 B가 동시에 포함된 거래 수}{A를 포함하는 거래 수}$\n",
    "-   향상도\n",
    "    -   규칙이 우연에 의해 발생한 것인지를 판단하 위해 연관성의 정도를 측정하는 척도\n",
    "    -   $\\frac{신뢰도}{P(B)}$\n",
    "    -   향상도 = 1 -> 서로 독립적 관계\n",
    "    -   향상도 > 1 -> 양의 상관관계\n",
    "    -   향상도 < 1 -> 음의 상관관계\n",
    "\n",
    "4) 연관성 분석 알고즘\n",
    "\n",
    "-   아프리오리 알고리즘\n",
    "    -   가능한 모든 경우의 수를 탐색하는 방식을 개선하 위하여 데이터들의 발생빈도가 높은 것을 찾는 알고리즘\n",
    "    -   분석 대상이 되는 항목의 대상을 최소화하여 큰 지지도 값을 갖는 빈발 항목 집합에 대해서만 연관규칙을 계산하는 알고리즘\n",
    "    -   한 항목이 자주 발생하지 않는다면 이 항목을 포함하는 집합들도 자주 발생하지 않는다는 규칙을 적용\n",
    "    -   계산 방법\n",
    "        -   우선적으로 최소 지지도 경곗값을 정하고 후보항목 집합을 생성함\n",
    "        -   후보 항목 집합에서 최소 지지도 경계값을 넘는 빈발항목 집합을 찾는다\n",
    "-   FP-Growth 알고리즘\n",
    "    -   FP-Tree라는 구조를 통해 최소 지지도를 만족하는 빈발 아이템 집합을 추출하는 알고리즘\n",
    "    -   모든 후보 아이템 세트들에 대하여 반복적으로 계산하는 단점이 있는 아프리오리 알고리즘을 개선한 알고리즘\n",
    "    -   장점\n",
    "        -   Tree 구조기 때문에 아프리오리 알고리즘보다 계산 속도가 빠르고 DB에서 스캔하는 횟수도 적음\n",
    "    -   단점\n",
    "        -   아프리오리에 비해 설계하기 어렵고, 지지도 계산은 무조건 FP-Tree가 만들어져야 가능함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46c30c",
   "metadata": {},
   "source": [
    "### 군집 분석 \n",
    "\n",
    "1) 군집 분석(Cluster Analysis)\n",
    "\n",
    "-   여러 개의 변수값들로부터 유사성에만 기초하여 n개의 군집으로 집단화하여 집단의 특성을 분석하는 다변량 분석 기법\n",
    "-   레이블이 없는 데이터 세트의 요약정보를 추출하고, 요약 정보를 통해 전체 데이터 세트가 가지고 있는 특징을 발견\n",
    "\n",
    "2) 군집 분석의 가정\n",
    "\n",
    "-   군집 내에 속한 개체들의 특성은 동질적이고 서로 다른 군집에 속한 개체들 간의 특성은 이질적이다\n",
    "-   군집 내의 응집도는 최대화하고, 군집 간의 분리도는 최대화 함\n",
    "-   구조와 관계없이 개체 간 거리를 기준으로 분류함\n",
    "\n",
    "3) 군집 간의 거리 계산\n",
    "\n",
    "-   연속형 변수 거리\n",
    "    -   수학적 거리\n",
    "        -   유클리드 거리(Euclidian) : $ d(i,j) = \\sqrt{\\sum_{f=1}^{p}(x_{if} - x_{jf})^2}$\n",
    "        -   맨하탄 거리(Manhattan) : $ d(i,j) = \\sum_{f=1}^{p}|x_{if} - x_{jf})|$\n",
    "        -   민코프스티(Minkowskii) : $ d(i,j) = (\\sum_{f=1}^{p}(x_{if} - x_{jf})^m)^{1/m}$\n",
    "    -   통계적 거리\n",
    "        -   표준화 거리 : 변수의 측정 단위를 표준화한 거리\n",
    "        -   말할라노비스 거리 : 변수의 표준화와 함께 변수 간의 상관성을 동시에 고려한 통계적 거리\n",
    "-   명목형 변수 거리\n",
    "    -   단순 일치 계수 : $\\frac{매칭된 속성의 개수}{속성의 개수}$\n",
    "    -   자카드(Jarcard) 계수 : $ J(A, B) = \\frac{|A \\cap B|}{|A| + |B| - |A \\cap B|} $\n",
    "-   순서형 변수 거리\n",
    "    -   순위상관계수(Rank Correlation Coefficient) : 값에 순위를 매겨 그 순위에 대해 상관계수를 구하는 방법\n",
    "\n",
    "4) 계층적 군집 분석\n",
    "\n",
    "-   병합적 방법\n",
    "    -   작은 군집으로부터 시작하여 군집을 병합하는 방법\n",
    "    -   거리가 가까우면 유사성이 높음\n",
    "-   분할적 방법\n",
    "    -   큰 군집으로부터 출발하여 군집을 분리해 나가는 기법\n",
    "-   계통도\n",
    "    -   군집의 결과는 계통도 또는 덴드로그램의 형태로 결과가 주어지며 각 개체는 하나의 군집에만 속하게 된다\n",
    "    -   항목간 거리, 군집 간의 거리를 알 수 있고, 군집 내 항목 간 유사 정도를 파악함으로써 견고성을 해석 가능\n",
    "-   군집 간의 거리측정 방법\n",
    "    -   최단 연결법 : 두 군집 사이의 거리를 각 군집에서 하나씩 관측값을 뽑았을 때 나타날 수 있는 거리의 최솟값으로 측정해서 가장 유사성이 큰 군집으로 병합해 나가는 방법\n",
    "    -   최장 연결법 : 두 군집 사이의 거리를 각 군집에서 하나씩 관측값을 뽑았을 떄 나타날 수 있는 거리의 최댓값으로 측정하여 가장 유사성이 큰 군집으로 병합해 나가는 방법\n",
    "    -   중심 연결법 : 두 군집의 중심간의 거리를 측정, 군집 내 편차들의 제곱합을 고려하여 군집 간 정보 손실을 최소화하는 방향으로 군집을 형성\n",
    "    -   평균 연결법 : 모든 항목에 대한 거리 평균을 구하면서 가장 유사성이 큰 군집을 병합해 나가는 방법\n",
    "    -   와드 연결법 : 군집 내의 오차제곱합에 기초하여 군집을 수행하는 방법, 오차제곱합의 증가량이 최초가 되는 방향으로 군집을 형성\n",
    "\n",
    "5) 비계층적 군집 분석\n",
    "\n",
    "-   분할 기반 군집(K-means clustering)\n",
    "    -   데이터를 K개의 군집으로 묶는 알고리즘으로 K개 만큼 군집수를 초깃값으로 지정하고, 각 개체를 가까운 초깃값에 할당하여 군집을 형성하고 각 군집의 평균을 재계산하여 초깃값을 갱신하는 과정을 반복하여 K개의 최종군집을 형성\n",
    "    -   군집의 수(K)는 초매개변수로 미리 정해준다\n",
    "    -   K개의 초기 중심 값은 임의로 선택할 수 있으며 자료 중에서 임의로 설정 가능\n",
    "    -   절차\n",
    "        -   K개 객체 선택 : 초기 군집 중심으로 K개의 객체를 임의로 선택\n",
    "        -   할당 : 자료를 가장 가까운 군집 중심에 할당\n",
    "        -   중심 갱신 : 각 군집 내의 자료들의 평균을 계산하여 군집의 중심을 갱신\n",
    "        -   반복 : 군집 중심의 변화 거의 없을 때까지 반복\n",
    "    -   K값 선정 기법\n",
    "        -   엘보우(Elbow) 기법 : x축에 클러스터의 개수, y축에 SSE값을 두었을 때 기울기 완만한 부분을 선택\n",
    "        -   실루엣 기법 : 각 군집 간의 거리가 얼마나 분리되어 있는지를 나타내는 기법\n",
    "        -   덴드로그램\n",
    "-   분보 기반 군집(혼합 분포 군집)\n",
    "    -   K개의 모수적 모형의 가중합으로 표현되는 모집단 무형으로부터 나왔다는 가정하에서 자료로부 모수와 가중치를 추정\n",
    "    -   K개의 모형중 어느 모형으로부터 나왔을 확률이 높은지에 따라 군집의 분류가 이루어짐\n",
    "    -   확률분포를 도입하여 군집을 수행하고, 서로 다른 크기의 군집을 찾을 수 있음\n",
    "    -   가우시안 혼합 모델(GMM)\n",
    "        -   전체 데이의 확률분포가 K개의 가우안 분포의 선형결합으로 이루어졌음을 가정하고 각 분포에 속할 확률이 높은 데이터 간의 군집을 형성\n",
    "        -   K 개의 가우시안 분포 중에서 어디에 속하는 것이 최적인지 추정\n",
    "    -   EM알고리즘\n",
    "        -   E-단계에는 잠재변수 Z의 기대치를 계산\n",
    "        -   M-단계에ㄴ 잠재변수 Z의 기대치를 이용하여 매개변수 추정\n",
    "-   밀도 기반 군집(DUBSCAN)\n",
    "    -   DUBSCAN은 개체들의 밀도 계산을 기반으로 밀접하게 분포된 개체들끼리 그룹핑하는 알고리즘\n",
    "    -   군집 밀도에 따라서 군집을 서로 연결하기 때문에 기하학적 모양의 군집 분석이 가능\n",
    "    -   구성요소\n",
    "        -   중심점 : 주변 반경 내에 최소 데이터 개수 이상의 다른 데이터를 가지고 있는 데이터\n",
    "        -   이웃점 : 특정 데이터 주변 반경 내에 존재하는 다른 데이터\n",
    "        -   경계점 : 중심점은 아니지만, 중심점이 주변 반경내에 존재하는 데이터\n",
    "        -   잡음점 : 중심점도 아니고 경계점 조건도 만족하지 못하는 이웃점\n",
    "    -   절차\n",
    "        -   반경 내에 최소 점 이상이 존재도록 중심점을 식별한다\n",
    "        -   인접 그래프에서 중심점과 연결된 구성요소를 찾는다\n",
    "        -   종심점 외에 속하면 노이즈로 할당한다\n",
    "    -   장점\n",
    "        -   K-means 과 같이 클러스터의 수를 정하지 않아도 됨\n",
    "        -   클러스터의 밀도에 따라서 클러스터를 서로 연결하 때문에 기하학적인 모양을 갖는 군집을 발견 가능\n",
    "    -   단점\n",
    "        -   초매개변수를 설정하기 어려움\n",
    "        -   다양한 밀도를 가지거나, 차원이 크면 계산에 어려움이 존재\n",
    "-   그래프 기반 군집(SOM)\n",
    "    -   인공신경망의 자율학습방법에 의한 클러스터링 방법을 적용한 알고리즘\n",
    "    -   고차원의 데이터를 이해하기 쉬운 저차원의 뉴런으로 정렬하여 지도의 형태로 형상화한 비지도 신경망\n",
    "    -   입력층과 경쟁층으로 구성된다\n",
    "\n",
    "6) 군집분석의 활용\n",
    "\n",
    "-   세분화\n",
    "-   이상탐지\n",
    "-   분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc02ba7",
   "metadata": {},
   "source": [
    "## 고급 분석 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e60c7",
   "metadata": {},
   "source": [
    "### 범주형 자료 분석 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28cc513",
   "metadata": {},
   "source": [
    "### 다변량 분석 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf687e",
   "metadata": {},
   "source": [
    "### 주성분 분석 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be5ff5d",
   "metadata": {},
   "source": [
    "### 딥러닝 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885dda97",
   "metadata": {},
   "source": [
    "### 비정형 데이터 분석 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d226ec9a",
   "metadata": {},
   "source": [
    "### 앙상블 분석 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c148b2",
   "metadata": {},
   "source": [
    "### 비모수 통계 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
